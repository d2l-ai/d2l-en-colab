{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n",
    "!pip install git+https://github.com/d2l-ai/d2l-en # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, np, npx\n",
    "from mxnet.contrib import text\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "# Saved in the d2l package for later use\n",
    "d2l.DATA_HUB['wikitext-103'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-103-v1.zip', '0aec09a7537b58d4bb65362fee27650eeaba625a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep paragraphs with at least 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        raw = f.readlines()\n",
    "    data = [line.strip().lower().split(' . ')\n",
    "            for line in raw if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare NSP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def get_next_sentence(sentence, next_sentence, all_docs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # all_docs is a list of lists of lists\n",
    "        next_sentence = random.choice(random.choice(all_docs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def get_tokens_and_segments(tokens_a, tokens_b):\n",
    "    tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "    segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def get_nsp_data_from_doc(doc, all_docs, vocab, max_length):\n",
    "    nsp_data_from_doc = []\n",
    "    for i in range(len(doc) - 1):\n",
    "        tokens_a, tokens_b, is_next = get_next_sentence(\n",
    "            doc[i], doc[i + 1], all_docs)\n",
    "        # Consider 1 '[CLS]' token and 2 '[SEP]' tokens\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_length:\n",
    "             continue\n",
    "        tokens, segment_ids = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_doc.append((tokens, segment_ids, is_next))\n",
    "    return nsp_data_from_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MLM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def replace_mlm_tokens(tokens, candidate_mlm_pred_positions, num_mlm_preds,\n",
    "                       vocab):\n",
    "    # Make a new copy of tokens for the input of a masked language model,\n",
    "    # where the input may contain replaced '[MASK]' or random tokens\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    mlm_pred_positions_and_labels = []\n",
    "    # Shuffle for gettting 15% random tokens for prediction in the masked\n",
    "    # language model task\n",
    "    random.shuffle(candidate_mlm_pred_positions)\n",
    "\n",
    "    for mlm_pred_position in candidate_mlm_pred_positions:\n",
    "        if len(mlm_pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80% of the time: replace the word with the '[MASK]' token\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '[MASK]'\n",
    "        else:\n",
    "            # 10% of the time: keep the word unchanged\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10% of the time: replace the word with a random word\n",
    "            else:\n",
    "                masked_token = random.randint(0, len(vocab) - 1)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        mlm_pred_positions_and_labels.append(\n",
    "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, mlm_pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def create_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_mlm_pred_positions = []\n",
    "    # tokens is a list of strings\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Special tokens are not predicted in the masked language model task\n",
    "        if token in ['[CLS]', '[SEP]']:\n",
    "            continue\n",
    "        candidate_mlm_pred_positions.append(i)\n",
    "    # 15% of random tokens will be predicted in the masked language model task \n",
    "    num_mlm_preds = max(1, int(round(len(tokens) * 0.15)))\n",
    "    output_tokens, mlm_pred_positions_and_labels = replace_mlm_tokens(\n",
    "        tokens, candidate_mlm_pred_positions, num_mlm_preds, vocab)\n",
    "\n",
    "    mlm_pred_positions_and_labels = sorted(mlm_pred_positions_and_labels,\n",
    "                                           key=lambda x: x[0])\n",
    "    mlm_pred_positions = []\n",
    "    mlm_pred_labels = []\n",
    "    for mlm_pred_position, mlm_pred_label in mlm_pred_positions_and_labels:\n",
    "        mlm_pred_positions.append(mlm_pred_position)\n",
    "        mlm_pred_labels.append(mlm_pred_label)\n",
    "    return vocab[output_tokens], mlm_pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def convert_numpy(instances, max_length):\n",
    "    input_ids, segment_ids, masked_lm_positions, masked_lm_ids = [], [], [], []\n",
    "    masked_lm_weights, next_sentence_labels, valid_lengths = [], [], []\n",
    "    for instance in instances:\n",
    "        input_id = instance[0] + [0] * (max_length - len(instance[0]))\n",
    "        segment_id = instance[3] + [0] * (max_length - len(instance[3]))\n",
    "        masked_lm_position = instance[1] + [0] * (20 - len(instance[1]))\n",
    "        masked_lm_id = instance[2] + [0] * (20 - len(instance[2]))\n",
    "        masked_lm_weight = [1.0] * len(instance[2]) + [0.0] * (20 - len(instance[1]))\n",
    "        next_sentence_label = instance[4]\n",
    "        valid_length = len(instance[0])\n",
    "\n",
    "        input_ids.append(np.array(input_id, dtype='int32'))\n",
    "        segment_ids.append(np.array(segment_id, dtype='int32'))\n",
    "        masked_lm_positions.append(np.array(masked_lm_position, dtype='int32'))\n",
    "        masked_lm_ids.append(np.array(masked_lm_id, dtype='int32'))\n",
    "        masked_lm_weights.append(np.array(masked_lm_weight, dtype='float32'))\n",
    "        next_sentence_labels.append(np.array(next_sentence_label))\n",
    "        valid_lengths.append(np.array(valid_length))\n",
    "    return input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights,\\\n",
    "           next_sentence_labels, segment_ids, valid_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def create_training_instances(train_data, vocab, max_length):\n",
    "    instances = []\n",
    "    for i, doc in enumerate(train_data):\n",
    "        instances.extend(get_nsp_data_from_doc(doc, train_data, vocab, max_length))\n",
    "    instances = [(create_mlm_data_from_tokens(tokens, vocab) + (segment_ids, is_random_next))\n",
    "                 for (tokens, segment_ids, is_random_next) in instances]\n",
    "    input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights,\\\n",
    "           next_sentence_labels, segment_ids, valid_lengths = convert_numpy(instances, max_length)\n",
    "    return input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights,\\\n",
    "           next_sentence_labels, segment_ids, valid_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class WikiDataset(gluon.data.Dataset):\n",
    "    def __init__(self, dataset, max_length = 128):\n",
    "        train_tokens = [d2l.tokenize(row, token='word') for row in dataset]\n",
    "        \n",
    "        text_list=[]\n",
    "        [text_list.extend(row) for row in train_tokens]\n",
    "        self.vocab = d2l.Vocab(text_list, min_freq=5, \n",
    "                               reserved_tokens=['[MASK]', '[CLS]', '[SEP]'])\n",
    "        self.input_ids, self.masked_lm_ids, self.masked_lm_positions,\\\n",
    "        self.masked_lm_weights, self.next_sentence_labels, self.segment_ids,\\\n",
    "        self.valid_lengths = create_training_instances(train_tokens, self.vocab, max_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.masked_lm_ids[idx], self.masked_lm_positions[idx], self.masked_lm_weights[idx],\\\n",
    "           self.next_sentence_labels[idx], self.segment_ids[idx], self.valid_lengths[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def load_data_wiki(batch_size, data_set = 'wikitext-2', num_steps=128):\n",
    "    data_dir = d2l.download_extract(data_set, data_set)\n",
    "    train_data = read_wiki(data_dir)\n",
    "    train_set = WikiDataset(train_data, num_steps)\n",
    "    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "train_iter, vocab = load_data_wiki(batch_size, 'wikitext-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 128) (512, 20) (512, 20) (512, 20) (512,) (512, 128) (512,)\n"
     ]
    }
   ],
   "source": [
    "for _, data_batch in enumerate(train_iter):\n",
    "    (input_id, masked_id, masked_position, masked_weight, \\\n",
    "     next_sentence_label, segment_id, valid_length) = data_batch\n",
    "    print(input_id.shape, masked_id.shape, masked_position.shape, masked_weight.shape,\\\n",
    "          next_sentence_label.shape, segment_id.shape, valid_length.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(len(vocab), embed_size=128, pw_num_hiddens=256, \n",
    "                    num_heads=2, num_layers=2, dropout=0.2)\n",
    "ctx = d2l.try_all_gpus()\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "nsp_loss = mx.gluon.loss.SoftmaxCELoss()\n",
    "mlm_loss = mx.gluon.loss.SoftmaxCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def _get_batch_bert(batch, ctx):\n",
    "    (input_id, masked_id, masked_position, masked_weight, \\\n",
    "     next_sentence_label, segment_id, valid_length) = batch\n",
    "    \n",
    "    return (gluon.utils.split_and_load(input_id, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(masked_id, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(masked_position, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(masked_weight, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(next_sentence_label, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(segment_id, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(valid_length.astype('float32'), ctx, even_split=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def batch_loss_bert(net, nsp_loss, mlm_loss, input_id, masked_id, masked_position,\n",
    "                    masked_weight, next_sentence_label, segment_id, valid_length, vocab_size):\n",
    "    ls = []\n",
    "    ls_mlm = []\n",
    "    ls_nsp = []\n",
    "    for i_id, m_id, m_pos, m_w, nsl, s_i, v_l in zip(input_id, masked_id, masked_position, masked_weight,\\\n",
    "                                                      next_sentence_label, segment_id, valid_length):\n",
    "        num_masks = m_w.sum() + 1e-8\n",
    "        _, decoded, classified = net(i_id, s_i, v_l.reshape(-1),m_pos)\n",
    "        l_mlm = mlm_loss(decoded.reshape((-1, vocab_size)),m_id.reshape(-1), m_w.reshape((-1, 1)))\n",
    "        l_mlm = l_mlm.sum() / num_masks\n",
    "        l_nsp = nsp_loss(classified, nsl)\n",
    "        l_nsp = l_nsp.mean()\n",
    "        l = l_mlm + l_nsp\n",
    "        ls.append(l)\n",
    "        ls_mlm.append(l_mlm)\n",
    "        ls_nsp.append(l_nsp)\n",
    "        npx.waitall()\n",
    "    return ls, ls_mlm, ls_nsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def train_bert(data_eval, net, nsp_loss, mlm_loss, vocab_size, ctx, log_interval, max_step):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam')\n",
    "    step_num = 0\n",
    "    while step_num < max_step:\n",
    "        eval_begin_time = time.time()\n",
    "        begin_time = time.time()\n",
    "        \n",
    "        running_mlm_loss = running_nsp_loss = 0\n",
    "        total_mlm_loss = total_nsp_loss = 0\n",
    "        running_num_tks = 0\n",
    "        for _, data_batch in enumerate(data_eval):\n",
    "            (input_id, masked_id, masked_position, masked_weight, \\\n",
    "             next_sentence_label, segment_id, valid_length) = _get_batch_bert(data_batch, ctx)\n",
    "            \n",
    "            step_num += 1\n",
    "            with autograd.record():\n",
    "                ls, ls_mlm, ls_nsp = batch_loss_bert(net, nsp_loss, mlm_loss, input_id, masked_id, masked_position, masked_weight, next_sentence_label, segment_id, valid_length, vocab_size)\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "            \n",
    "            trainer.step(1)\n",
    "            \n",
    "            running_mlm_loss += sum([l for l in ls_mlm])\n",
    "            running_nsp_loss += sum([l for l in ls_nsp])\n",
    "\n",
    "            if (step_num + 1) % (log_interval) == 0:\n",
    "                total_mlm_loss += running_mlm_loss\n",
    "                total_nsp_loss += running_nsp_loss\n",
    "                begin_time = time.time()\n",
    "                running_mlm_loss = running_nsp_loss = 0\n",
    "\n",
    "        eval_end_time = time.time()\n",
    "        if running_mlm_loss != 0:\n",
    "            total_mlm_loss += running_mlm_loss\n",
    "            total_nsp_loss += running_nsp_loss\n",
    "        total_mlm_loss /= step_num\n",
    "        total_nsp_loss /= step_num\n",
    "        print('Eval mlm_loss={:.3f}\\tnsp_loss={:.3f}\\t'\n",
    "                     .format(float(total_mlm_loss),\n",
    "                             float(total_nsp_loss)))\n",
    "        print('Eval cost={:.1f}s'.format(eval_end_time - eval_begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval mlm_loss=14.657\tnsp_loss=1.449\t\n",
      "Eval cost=8.2s\n"
     ]
    }
   ],
   "source": [
    "train_bert(train_iter, net, nsp_loss, mlm_loss, len(vocab), ctx, 20, 1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}