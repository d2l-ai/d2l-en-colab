{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n",
    "!pip install git+https://github.com/d2l-ai/d2l-en # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining BERT\n",
    "\n",
    "*This section is under construction.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, np, npx\n",
    "from mxnet.contrib import text\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep paragraphs with at least 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # A line represents a paragragh.\n",
    "    paragraghs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraghs)\n",
    "    return paragraghs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare NSP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # paragraphs is a list of lists of lists\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def get_tokens_and_segments(tokens_a, tokens_b):\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>'] + tokens_b + ['<sep>']\n",
    "    segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "             continue\n",
    "        tokens, segment_ids = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segment_ids, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MLM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def replace_mlm_tokens(tokens, candidate_mlm_pred_positions, num_mlm_preds,\n",
    "                       vocab):\n",
    "    # Make a new copy of tokens for the input of a masked language model,\n",
    "    # where the input may contain replaced '<mask>' or random tokens\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    mlm_pred_positions_and_labels = []\n",
    "    # Shuffle for gettting 15% random tokens for prediction in the masked\n",
    "    # language model task\n",
    "    random.shuffle(candidate_mlm_pred_positions)\n",
    "    for mlm_pred_position in candidate_mlm_pred_positions:\n",
    "        if len(mlm_pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80% of the time: replace the word with the '<mask>' token\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10% of the time: keep the word unchanged\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10% of the time: replace the word with a random word\n",
    "            else:\n",
    "                masked_token = random.randint(0, len(vocab) - 1)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        mlm_pred_positions_and_labels.append(\n",
    "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, mlm_pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_mlm_pred_positions = []\n",
    "    # tokens is a list of strings\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Special tokens are not predicted in the masked language model task\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_mlm_pred_positions.append(i)\n",
    "    # 15% of random tokens will be predicted in the masked language model task\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, mlm_pred_positions_and_labels = replace_mlm_tokens(\n",
    "        tokens, candidate_mlm_pred_positions, num_mlm_preds, vocab)\n",
    "    mlm_pred_positions_and_labels = sorted(mlm_pred_positions_and_labels,\n",
    "                                           key=lambda x: x[0])\n",
    "    \n",
    "    zipped_positions_and_labels = list(zip(*mlm_pred_positions_and_labels))\n",
    "    # e.g., [[1, 'an'], [12, 'car'], [25, '<unk>']] -> [1, 12, 25]\n",
    "    mlm_pred_positions = list(zipped_positions_and_labels[0])\n",
    "    # e.g., [[1, 'an'], [12, 'car'], [25, '<unk>']] -> ['an', 'car', '<unk>']\n",
    "    mlm_pred_labels = list(zipped_positions_and_labels[1])\n",
    "    return vocab[mlm_input_tokens], mlm_pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def pad_bert_inputs(instances, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    X_mlm_tokens, X_segments, valid_lens = [], [], []\n",
    "    X_mlm_pred_positions, X_mlm_weights, Y_mlm, y_nsp = [], [], [], []\n",
    "    for (mlm_input_ids, mlm_pred_positions, mlm_pred_label_ids, segment_ids,\n",
    "         is_next) in instances:\n",
    "        X_mlm_tokens.append(np.array(mlm_input_ids + [vocab['<pad>']] * (\n",
    "            max_len - len(mlm_input_ids)), dtype='int32'))\n",
    "        X_segments.append(np.array(segment_ids + [0] * (\n",
    "            max_len - len(segment_ids)), dtype='int32'))\n",
    "        valid_lens.append(np.array(len(mlm_input_ids)))\n",
    "        X_mlm_pred_positions.append(np.array(mlm_pred_positions + [0] * (\n",
    "            20 - len(mlm_pred_positions)), dtype='int32'))\n",
    "        # Predictions of padded tokens will be filtered out in the loss via\n",
    "        # multiplication of 0 weights\n",
    "        X_mlm_weights.append(np.array([1.0] * len(mlm_pred_label_ids) + [\n",
    "            0.0] * (20 - len(mlm_pred_positions)), dtype='float32'))\n",
    "        Y_mlm.append(np.array(mlm_pred_label_ids + [0] * (\n",
    "            20 - len(mlm_pred_label_ids)), dtype='int32'))\n",
    "        y_nsp.append(np.array(is_next))\n",
    "    return (X_mlm_tokens, X_segments, valid_lens, X_mlm_pred_positions,\n",
    "            X_mlm_weights, Y_mlm, y_nsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class WikiTextDataset(gluon.data.Dataset):\n",
    "    def __init__(self, paragraghs, max_len=128):\n",
    "        # Input paragraghs[i] is a list of sentence strings representing a\n",
    "        # paragraph; while output paragraghs[i] is a list of sentences\n",
    "        # representing a paragraph, where each sentence is a list of tokens\n",
    "        paragraghs = [d2l.tokenize(\n",
    "            paragraph, token='word') for paragraph in paragraghs]\n",
    "        sentences = [sentence for paragraph in paragraghs\n",
    "                     for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
    "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # Get data for the next sentence prediction task\n",
    "        instances = []\n",
    "        for paragraph in paragraghs:\n",
    "            instances.extend(get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraghs, self.vocab, max_len))\n",
    "        # Get data for the masked language model task \n",
    "        instances = [(get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                      + (segment_ids, is_next))\n",
    "                     for tokens, segment_ids, is_next in instances]\n",
    "        # Pad inputs\n",
    "        (self.X_mlm_tokens, self.X_segments, self.valid_lens,\n",
    "         self.X_mlm_pred_positions, self.X_mlm_weights, self.Y_mlm,\n",
    "         self.y_nsp) = pad_bert_inputs(instances, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X_mlm_tokens[idx], self.X_segments[idx],\n",
    "                self.valid_lens[idx], self.X_mlm_pred_positions[idx],\n",
    "                self.X_mlm_weights[idx], self.Y_mlm[idx], self.y_nsp[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mlm_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    paragraghs = read_wiki(data_dir)\n",
    "    train_set = WikiTextDataset(paragraghs, max_len)\n",
    "    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 128\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 128) (512, 128) (512,) (512, 20) (512, 20) (512, 20) (512,)\n"
     ]
    }
   ],
   "source": [
    "for (X_mlm_tokens, X_segments, valid_lens, X_mlm_pred_positions,\n",
    "     X_mlm_weights, Y_mlm, y_nsp) in train_iter:\n",
    "    print(X_mlm_tokens.shape, X_segments.shape, valid_lens.shape,\n",
    "          X_mlm_pred_positions.shape, X_mlm_weights.shape, Y_mlm.shape,\n",
    "          y_nsp.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(len(vocab), embed_size=128, pw_num_hiddens=256,\n",
    "                    num_heads=2, num_layers=2, dropout=0.2)\n",
    "ctx = d2l.try_all_gpus()\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "nsp_loss, mlm_loss = gluon.loss.SoftmaxCELoss(), gluon.loss.SoftmaxCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def _get_batch_bert(batch, ctx):\n",
    "    (X_mlm_tokens, X_segments, valid_lens, X_mlm_pred_positions,\n",
    "     X_mlm_weights, Y_mlm, y_nsp) = batch\n",
    "    split_and_load = gluon.utils.split_and_load\n",
    "    return (split_and_load(X_mlm_tokens, ctx, even_split=False),\n",
    "            split_and_load(X_segments, ctx, even_split=False),\n",
    "            split_and_load(valid_lens.astype('float32'), ctx,\n",
    "                           even_split=False),\n",
    "            split_and_load(X_mlm_pred_positions, ctx, even_split=False),\n",
    "            split_and_load(X_mlm_weights, ctx, even_split=False),\n",
    "            split_and_load(Y_mlm, ctx, even_split=False),\n",
    "            split_and_load(y_nsp, ctx, even_split=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def batch_loss_bert(net, nsp_loss, mlm_loss, X_mlm_tokens_shards,\n",
    "                    X_segments_shards, valid_lens_shards,\n",
    "                    X_mlm_pred_positions_shards, X_mlm_weights_shards,\n",
    "                    Y_mlm_shards, y_nsp_shards, vocab_size):\n",
    "    ls = []\n",
    "    ls_mlm = []\n",
    "    ls_nsp = []\n",
    "    for (X_mlm_tokens_shard, X_segments_shard, valid_lens_shard,\n",
    "         X_mlm_pred_positions_shard, X_mlm_weights_shard, Y_mlm_shard,\n",
    "         y_nsp_shard) in zip(\n",
    "        X_mlm_tokens_shards, X_segments_shards, valid_lens_shards,\n",
    "        X_mlm_pred_positions_shards, X_mlm_weights_shards, Y_mlm_shards,\n",
    "        y_nsp_shards):\n",
    "\n",
    "        num_masks = X_mlm_weights_shard.sum() + 1e-8\n",
    "        _, decoded, classified = net(\n",
    "            X_mlm_tokens_shard, X_segments_shard,\n",
    "            valid_lens_shard.reshape(-1), X_mlm_pred_positions_shard)\n",
    "        l_mlm = mlm_loss(\n",
    "            decoded.reshape((-1, vocab_size)), Y_mlm_shard.reshape(-1),\n",
    "            X_mlm_weights_shard.reshape((-1, 1)))\n",
    "        l_mlm = l_mlm.sum() / num_masks\n",
    "        l_nsp = nsp_loss(classified, y_nsp_shard)\n",
    "        l_nsp = l_nsp.mean()\n",
    "        l = l_mlm + l_nsp\n",
    "        ls.append(l)\n",
    "        ls_mlm.append(l_mlm)\n",
    "        ls_nsp.append(l_nsp)\n",
    "        npx.waitall()\n",
    "    return ls, ls_mlm, ls_nsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def train_bert(data_eval, net, nsp_loss, mlm_loss, vocab_size, ctx,\n",
    "               log_interval, max_step):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam')\n",
    "    step_num = 0\n",
    "    while step_num < max_step:\n",
    "        eval_begin_time = time.time()\n",
    "        begin_time = time.time()\n",
    "\n",
    "        running_mlm_loss = running_nsp_loss = 0\n",
    "        total_mlm_loss = total_nsp_loss = 0\n",
    "        running_num_tks = 0\n",
    "        for _, data_batch in enumerate(data_eval):\n",
    "            (X_mlm_tokens_shards, X_segments_shards, valid_lens_shards,\n",
    "             X_mlm_pred_positions_shards, X_mlm_weights_shards,\n",
    "             Y_mlm_shards, y_nsp_shards) = _get_batch_bert(data_batch, ctx)\n",
    "\n",
    "            step_num += 1\n",
    "            with autograd.record():\n",
    "                ls, ls_mlm, ls_nsp = batch_loss_bert(\n",
    "                    net, nsp_loss, mlm_loss, X_mlm_tokens_shards,\n",
    "                    X_segments_shards, valid_lens_shards,\n",
    "                    X_mlm_pred_positions_shards, X_mlm_weights_shards,\n",
    "                    Y_mlm_shards, y_nsp_shards, vocab_size)\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            running_mlm_loss += sum([l for l in ls_mlm])\n",
    "            running_nsp_loss += sum([l for l in ls_nsp])\n",
    "\n",
    "            if (step_num + 1) % (log_interval) == 0:\n",
    "                total_mlm_loss += running_mlm_loss\n",
    "                total_nsp_loss += running_nsp_loss\n",
    "                begin_time = time.time()\n",
    "                running_mlm_loss = running_nsp_loss = 0\n",
    "\n",
    "        eval_end_time = time.time()\n",
    "        if running_mlm_loss != 0:\n",
    "            total_mlm_loss += running_mlm_loss\n",
    "            total_nsp_loss += running_nsp_loss\n",
    "        total_mlm_loss /= step_num\n",
    "        total_nsp_loss /= step_num\n",
    "        print('Eval mlm_loss={:.3f}\\tnsp_loss={:.3f}\\t'\n",
    "                     .format(float(total_mlm_loss),\n",
    "                             float(total_nsp_loss)))\n",
    "        print('Eval cost={:.1f}s'.format(eval_end_time - eval_begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval mlm_loss=14.679\tnsp_loss=1.483\t\n",
      "Eval cost=7.6s\n"
     ]
    }
   ],
   "source": [
    "train_bert(train_iter, net, nsp_loss, mlm_loss, len(vocab), ctx, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try other sentence segmentation methods, such as `spaCy` and `nltk.tokenize.sent_tokenize`. For instance, after installing `nltk`, you need to run `import nltk` and `nltk.download('punkt')` first."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}