{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n",
    "!pip install ..  # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Multilayer Perceptron from Scratch\n",
    "\n",
    ":label:`sec_mlp_scratch`\n",
    "\n",
    "\n",
    "Now that we know how multilayer perceptrons (MLPs) work in theory,\n",
    "let's implement them. First, we import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare against the results\n",
    "we previously achieved with vanilla softmax regression,\n",
    "we continue to use the Fashion-MNIST image classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Model Parameters\n",
    "\n",
    "Recall that this dataset contains 10 classes and that\n",
    "each image consists of a $28 \\times 28 = 784$ grid of pixel values.\n",
    "Since we will be discarding the spatial structure (for now),\n",
    "we can just think of this as a classification dataset\n",
    "with $784$ input features and $10$ classes.\n",
    "In particular we will implement our MLP\n",
    "with one hidden layer and $256$ hidden units.\n",
    "Note that we can regard both of these choices as *hyperparameters*\n",
    "that could be set based on performance on validation data.\n",
    "Typically, we will choose layer widths as powers of $2$\n",
    "to make everything align nicely in memory.\n",
    "\n",
    "Again, we will represent our parameters with several `ndarray`s.\n",
    "Note that we now have one weight matrix and one bias vector *per layer*.\n",
    "As always, we must call `attach_grad` to allocate memory for the gradients with respect to these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens))\n",
    "b1 = np.zeros(num_hiddens)\n",
    "W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs))\n",
    "b2 = np.zeros(num_outputs)\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "To make sure we know how everything works,\n",
    "we will use the `maximum` function to implement ReLU ourselves,\n",
    "instead of invoking `npx.relu` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "As in softmax regression, we will `reshape` each 2D image\n",
    "into a flat vector of length  `num_inputs`.\n",
    "Finally, we can implement our model with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape(-1, num_inputs)\n",
    "    H = relu(np.dot(X, W1) + b1)\n",
    "    return np.dot(H, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "\n",
    "For better numerical stability and because we already know\n",
    "how to implement softmax regression completely from scratch in :numref:`sec_softmax_scratch`,\n",
    "we will use Gluon's integrated function\n",
    "for calculating the softmax and cross-entropy loss.\n",
    "Recall that we discussed some of these intricacies\n",
    "in :numref:`sec_mlp`.\n",
    "We encourage the interested reader to examing the source code\n",
    "for `mxnet.gluon.loss.SoftmaxCrossEntropyLoss` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Steps for training the MLP are no different than for softmax regression.\n",
    "In the `d2l` package, we directly call the `train_ch3` function, whose implementation was introduced in :numref:`sec_softmax_scratch`.\n",
    "We set the number of epochs to $10$ and the learning rate to $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs, lr = 10, 0.5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,\n",
    "              lambda batch_size: d2l.sgd(params, lr, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well we did, let's apply the model to some test data.\n",
    "If you are interested, compare the result to corresponding linear model in :numref:`sec_softmax_scratch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.predict_ch3(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a bit better than our previous result, a good sign that we are on the right path.\n",
    "\n",
    "## Summary\n",
    "\n",
    "We saw that implementing a simple MLP is easy, even when done manually.\n",
    "That said, with a large number of layers, this can get messy\n",
    "(e.g., naming and keeping track of the model parameters, etc).\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Change the value of the hyperparameter `num_hiddens` in order to see how this hyperparameter influences your results.\n",
    "1. Try adding a new hidden layer to see how it affects the results.\n",
    "1. How does changing the learning rate change the result?\n",
    "1. What is the best result you can get by optimizing over all the parameters (learning rate, iterations, number of hidden layers, number of hidden units per layer)?\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/2339)\n",
    "\n",
    "![](../img/qr_mlp-scratch.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}