{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n",
    "!pip install ..  # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation and the Dataset\n",
    "\n",
    ":label:`sec_machine_translation`\n",
    "\n",
    "\n",
    "So far we see how to use recurrent neural networks for language models, in which we predict the next token given all previous tokens in an article. Now let's have a look at a different application, machine translation, whose predict output is no longer a single token, but a list of tokens. \n",
    "\n",
    "Machine translation (MT) refers to the automatic translation of a segment of text from one language to another. Solving this problem with neural networks is often called neural machine translation (NMT). Compared to language models (:numref:`sec_language_model`), in which the corpus only contains a single language, machine translation dataset has at least two languages, the source language and the target language. In addition, each sentence in the source language is mapped to the according translation in the target language. Therefore, the data preprocessing for machine translation data is different to the one for language models. This section is dedicated to demonstrate how to pre-process such a dataset and then load into a set of minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "import zipfile\n",
    "\n",
    "from mxnet import np, npx, gluon\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing the Dataset\n",
    "\n",
    "We first download a dataset that contains a set of English sentences with the corresponding French translations. As can be seen that each line contains a English sentence with its French translation, which are separated by a `TAB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def read_data_nmt():\n",
    "    fname = gluon.utils.download('http://data.mxnet.io/data/fra-eng.zip')\n",
    "    with zipfile.ZipFile(fname, 'r') as f:\n",
    "        return f.read('fra.txt').decode(\"utf-8\")\n",
    "\n",
    "raw_text = read_data_nmt()\n",
    "print(raw_text[0:106])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform several preprocessing steps on the raw text data, including ignoring cases, replacing UTF-8 non-breaking space with space, and adding space between words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def preprocess_nmt(text):\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "\n",
    "    def no_space(char, prev_char):\n",
    "        return (True if char in (',', '!', '.')\n",
    "                and prev_char != ' ' else False)\n",
    "    \n",
    "    out = [' '+char if i > 0 and no_space(char, text[i-1]) else char\n",
    "           for i, char in enumerate(text.lower())]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[0:95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Different to using character tokens in :numref:`sec_language_model`, here a token is either a word or a punctuation mark. The following function tokenizes the text data to return `source` and `target`. Each one is a list of token list, with `source[i]` is the $i^\\mathrm{th}$ sentence in the source language and `target[i]` is the $i^\\mathrm{th}$ sentence in the target language. To make the latter training faster, we sample the first `num_examples` sentences pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def tokenize_nmt(text, num_examples=None):\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "source[0:3], target[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the histogram of the number of tokens per sentence the following figure. As can be seen that a sentence in average contains 5 tokens, and most of them have less than 10 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "d2l.set_figsize((3.5, 2.5))\n",
    "d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],\n",
    "             label=['source', 'target'])\n",
    "d2l.plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Since the tokens in the source language could be different to the ones in the target language, we need to build a vocabulary for each of them. Since we are using words instead of characters  as tokens, it makes the vocabulary size significantly large. Here we map every token that appears less than 3 times into the &lt;unk&gt; token :numref:`sec_text_preprocessing`. In addition, we need other special tokens such as padding and sentence beginnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "src_vocab = d2l.Vocab(source, min_freq=3, use_special_tokens=True)\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "In language models, each example is a `num_steps` length sequence from the corpus, which may be a segment of a sentence, or span over multiple sentences. In machine translation, an example should contain a pair of source sentence and target sentence. These sentences might have different lengths, while we need same length examples to form a minibatch. \n",
    "\n",
    "One way to solve this problem is that if a sentence is longer than `num_steps`, we trim it is length, otherwise pad with a special &lt;pad&gt; token to meet the length. Therefore we could transform any sentence to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def trim_pad(line, num_steps, padding_token):\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Trim\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
    "\n",
    "\n",
    "trim_pad(src_vocab[source[0]], 10, src_vocab.pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert a list of sentences into an `(num_example, num_steps)` index array. We also record the length of each sentence without the padding tokens, called *valid length*, which might be used by some models. In addition, we add the special “&lt;bos&gt;” and “&lt;eos&gt;” tokens to the target sentences so that our model will know the signals for starting and ending predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def build_array(lines, vocab, num_steps, is_source):\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    if not is_source:\n",
    "        lines = [[vocab.bos] + l + [vocab.eos] for l in lines]\n",
    "    array = np.array([trim_pad(l, num_steps, vocab.pad) for l in lines])\n",
    "    valid_len = (array != vocab.pad).sum(axis=1)\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can construct minibatches based on these arrays. \n",
    "\n",
    "## Putting All Things Together\n",
    "\n",
    "Finally, we define the function `load_data_nmt` to return the data iterator with the vocabularies for source language and target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def load_data_nmt(batch_size, num_steps, num_examples=1000):\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = d2l.Vocab(source, min_freq=3, use_special_tokens=True)\n",
    "    tgt_vocab = d2l.Vocab(target, min_freq=3, use_special_tokens=True)\n",
    "    src_array, src_valid_len = build_array(\n",
    "        source, src_vocab, num_steps, True)\n",
    "    tgt_array, tgt_valid_len = build_array(\n",
    "        target, tgt_vocab, num_steps, False)\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
    "    return src_vocab, tgt_vocab, data_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, num_steps=8)\n",
    "for X, X_vlen, Y, Y_vlen, in train_iter:\n",
    "    print('X =', X.astype('int32'), '\\nValid lengths for X =', X_vlen,\n",
    "          '\\nY =', Y.astype('int32'), '\\nValid lengths for Y =', Y_vlen)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Machine translation (MT) refers to the automatic translation of a segment of text from one language to another. \n",
    "* We read, preprocess, and tokenize the datasets from both source language and target language.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Find a machine translation dataset online and process it.\n",
    "\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/machine-translation/2396)\n",
    "\n",
    "![](../img/qr_machine-translation.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}