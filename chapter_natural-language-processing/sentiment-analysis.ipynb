{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n",
    "!pip install ..  # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification and the Dataset\n",
    "\n",
    ":label:`sec_sentiment`\n",
    "\n",
    "\n",
    "Text classification is a common task in natural language processing, which transforms a sequence of text of indefinite length into a category of text. It is similar to the image classification, the most frequently used application in this book, e.g., :numref:`sec_naive_bayes`. The only difference is that, rather than an image, text classification's example is a text sentence. \n",
    "\n",
    "This section will focus on loading data for one of the sub-questions in this field: using text sentiment classification to analyze the emotions of the text's author. This problem is also called sentiment analysis and has a wide range of applications. For example, we can analyze user reviews of products to obtain user satisfaction statistics, or analyze user sentiments about market conditions and use it to predict future trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, np, npx\n",
    "import os\n",
    "import tarfile\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Text Sentiment Classification Dataset\n",
    "\n",
    "We use Stanford's Large Movie Review Dataset as the dataset for text sentiment classification[1]. This dataset is divided into two datasets for training and testing purposes, each containing 25,000 movie reviews downloaded from IMDb. In each dataset, the number of comments labeled as \"positive\" and \"negative\" is equal.\n",
    "\n",
    "###  Reading the Dataset\n",
    "\n",
    "We first download this dataset to the \"../data\" path and extract it to \"../data/aclImdb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def download_imdb(data_dir='../data'):\n",
    "    url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    fname = gluon.utils.download(url, data_dir)\n",
    "    with tarfile.open(fname, 'r') as f:\n",
    "        f.extractall(data_dir)\n",
    "\n",
    "download_imdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read the training and test datasets. Each example is a review and its corresponding label: 1 indicates \"positive\" and 0 indicates \"negative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def read_imdb(folder='train', data_dir='../data'):\n",
    "    data, labels = [], []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_dir, 'aclImdb', folder, label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_data = read_imdb('train')\n",
    "print('# trainings:', len(train_data[0]))\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('label:', y, 'review:', x[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Vocabulary\n",
    "\n",
    "We use a word as a token, and then create a dictionary based on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "\n",
    "d2l.set_figsize((3.5, 2.5))\n",
    "d2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding to the Same Length\n",
    "\n",
    "Because the reviews have different lengths, so they cannot be directly combined into minibatches. Here we fix the length of each comment to 500 by truncating or adding \"&lt;unk&gt;\" indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "num_steps = 500  # sequence length\n",
    "train_features = np.array([d2l.trim_pad(vocab[line], num_steps, vocab.unk)\n",
    "                           for line in train_tokens])\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Data Iterator\n",
    "\n",
    "Now, we will create a data iterator. Each iteration will return a minibatch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = d2l.load_array((train_features, train_data[1]), 64)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'# batches:', len(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting All Things Together\n",
    "\n",
    "Last, we will save a function `load_data_imdb` into `d2l`, which returns the vocabulary and data iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    download_imdb()\n",
    "    train_data, test_data = read_imdb('train'), read_imdb('test')\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "    train_features = np.array([d2l.trim_pad(vocab[line], num_steps, vocab.unk)\n",
    "                               for line in train_tokens])\n",
    "    test_features = np.array([d2l.trim_pad(vocab[line], num_steps, vocab.unk)\n",
    "                              for line in test_tokens])\n",
    "    train_iter = d2l.load_array((train_features, train_data[1]), batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_data[1]), batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Text classification can classify a text sequence into a category.\n",
    "* To classify a text sentiment, we load an IMDb dataset and tokenize its words. Then we pad the text sequence for short reviews and create a data iterator.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Discover a different natural language dataset (such as [Amazon reviews](https://snap.stanford.edu/data/web-Amazon.html)) and build a similar data_loader function as `load_data_imdb`.\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/4355)\n",
    "\n",
    "![](../img/qr_sentiment-analysis.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}