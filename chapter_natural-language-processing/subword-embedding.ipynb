{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Embedding\n",
    "\n",
    ":label:`sec_fasttext`\n",
    "\n",
    "\n",
    "English words usually have internal structures and formation methods. For example, we can deduce the relationship between \"dog\", \"dogs\", and \"dogcatcher\" by their spelling. All these words have the same root, \"dog\", but they use different suffixes to change the meaning of the word. Moreover, this association can be extended to other words. For example, the relationship between \"dog\" and \"dogs\" is just like the relationship between \"cat\" and \"cats\". The relationship between \"boy\" and \"boyfriend\" is just like the relationship between \"girl\" and \"girlfriend\". This characteristic is not unique to English. In French and Spanish, a lot of verbs can have more than 40 different forms depending on the context. In Finnish, a noun may have more than 15 forms. In fact, morphology, which is an important branch of linguistics, studies the internal structure and formation of words.\n",
    "\n",
    "\n",
    "## fastText\n",
    "\n",
    "In word2vec, we did not directly use morphology information.  In both the\n",
    "skip-gram model and continuous bag-of-words model, we use different vectors to\n",
    "represent words with different forms. For example, \"dog\" and \"dogs\" are\n",
    "represented by two different vectors, while the relationship between these two\n",
    "vectors is not directly represented in the model. In view of this, fastText :cite:`Bojanowski.Grave.Joulin.ea.2017`\n",
    "proposes the method of subword embedding, thereby attempting to introduce\n",
    "morphological information in the skip-gram model in word2vec.\n",
    "\n",
    "In fastText, each central word is represented as a collection of subwords. Below we use the word \"where\" as an example to understand how subwords are formed. First, we add the special characters “&lt;” and “&gt;” at the beginning and end of the word to distinguish the subwords used as prefixes and suffixes. Then, we treat the word as a sequence of characters to extract the $n$-grams. For example, when $n=3$, we can get all subwords with a length of $3$:\n",
    "\n",
    "$$\\textrm{\"<wh\"}, \\ \\textrm{\"whe\"}, \\ \\textrm{\"her\"}, \\ \\textrm{\"ere\"}, \\ \\textrm{\"re>\"},$$\n",
    "\n",
    "and the special subword $\\textrm{\"<where>\"}$.\n",
    "\n",
    "In fastText, for a word $w$, we record the union of all its subwords with length of $3$ to $6$ and special subwords as $\\mathcal{G}_w$. Thus, the dictionary is the union of the collection of subwords of all words. Assume the vector of the subword $g$ in the dictionary is $\\mathbf{z}_g$. Then, the central word vector $\\mathbf{u}_w$ for the word $w$ in the skip-gram model can be expressed as\n",
    "\n",
    "$$\\mathbf{u}_w = \\sum_{g\\in\\mathcal{G}_w} \\mathbf{z}_g.$$\n",
    "\n",
    "The rest of the fastText process is consistent with the skip-gram model, so it is not repeated here. As we can see, compared with the skip-gram model, the dictionary in fastText is larger, resulting in more model parameters. Also, the vector of one word requires the summation of all subword vectors, which results in higher computation complexity. However, we can obtain better vectors for more uncommon complex words, even words not existing in the dictionary, by looking at other words with similar structures.\n",
    "\n",
    "\n",
    "## Byte Pair Encoding\n",
    "\n",
    "In fastText, all the extracted subwords have to be of the specified lengths, such as $3$ to $6$, thus the vocabulary size cannot be predefined.\n",
    "To allow for variable-length subwords in a fixed-size vocabulary,\n",
    "we can apply a compression algorithm\n",
    "called *byte pair encoding* (BPE) to extract subwords :cite:`Sennrich.Haddow.Birch.2015`.\n",
    "Starting from symbols of length $1$,\n",
    "BPE iteratively merges the frequent pair of symbols, such as consecutive characters of\n",
    "arbitrary length within a word, to produce new symbols.\n",
    "Note that for efficiency, pairs crossing word boundaries are not considered.\n",
    "In the end, we can use such symbols as subwords to segment words.\n",
    "In the following, we will illustrate how BPE works.\n",
    "\n",
    "First, we initialize the vocabulary of symbols as all the English characters, a special end-of-word symbol `'_'`, and a special unknown symbol `'[UNK]'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "           '_', '[UNK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not consider symbol pairs that cross boundaries of words,\n",
    "we only need a dictionary `raw_token_freqs` that maps words to frequencies as the representation of a dataset.\n",
    "Note that the special symbol `'_'` is appended to each word so that\n",
    "we can easily convert a sequence of symbols ( e.g., \"a_ tall er_ man\")\n",
    "to its corresponding word sequence (e.g., \"a taller man\").\n",
    "Since we start the merging process from a vocabulary of only single characters and special symbols, space is inserted between every pair of consecutive characters within each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "token_freqs = {}\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_freq_pair(token_freqs):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Key of pairs is a tuple composed of two adjacent units\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return max(pairs, key=pairs.get)  # Key of pairs with the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(max_freq_pair, token_freqs, symbols):\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = {}\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                                  ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge #1: ('t', 'a')\n",
      "Merge #2: ('ta', 'l')\n",
      "Merge #3: ('tal', 'l')\n",
      "Merge #4: ('f', 'a')\n",
      "Merge #5: ('fa', 's')\n",
      "Merge #6: ('fas', 't')\n",
      "Merge #7: ('e', 'r')\n",
      "Merge #8: ('er', '_')\n",
      "Merge #9: ('tall', '_')\n",
      "Merge #10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_vocab(max_freq_pair, token_freqs, symbols)\n",
    "    print(\"Merge #%d:\" % (i + 1), max_freq_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n"
     ]
    }
   ],
   "source": [
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['fast_', 'faster_', 'tall_', 'taller_']\n",
      "Tokens with BPE segmentation: ['fast_', 'fast er_', 'tall_', 'tall er_']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original tokens:\", list(raw_token_freqs.keys()))\n",
    "print(\"Tokens with BPE segmentation:\", list(token_freqs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['tallest_', 'fatter_']\n",
      "Wordpieces: ['tall e s t _', 'fa t t er_']\n"
     ]
    }
   ],
   "source": [
    "inputs = ['tallest_', 'fatter_']\n",
    "outputs = []\n",
    "for word in inputs:\n",
    "    start, end = 0, len(word)\n",
    "    cur_output = []\n",
    "    while start < len(word) and start < end:\n",
    "        if word[start : end] in symbols:\n",
    "            cur_output.append(word[start : end])\n",
    "            start = end\n",
    "            end = len(word)\n",
    "        else:\n",
    "            end -= 1\n",
    "    if start < len(word):\n",
    "        cur_output.append('[UNK]')\n",
    "    outputs.append(' '.join(cur_output))\n",
    "print('Words:', inputs)\n",
    "print('Wordpieces:', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* FastText proposes a subword embedding method. Based on the skip-gram model in word2vec, it represents the central word vector as the sum of the subword vectors of the word.\n",
    "* Subword embedding utilizes the principles of morphology, which usually improves the quality of representations of uncommon words.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. When there are too many subwords (for example, 6 words in English result in about $3\\times 10^8$ combinations), what problems arise? Can you think of any methods to solve them? Hint: Refer to the end of section 3.2 of the fastText paper[1].\n",
    "1. How can you design a subword embedding model based on the continuous bag-of-words model?\n",
    "1. To get a vocabulary of size $m$, how many merging operations are needed when the initial symbol vocabulary size is $n$?\n",
    "\n",
    "\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/2388)\n",
    "\n",
    "![](../img/qr_subword-embedding.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}