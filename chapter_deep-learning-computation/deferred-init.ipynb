{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deferred Initialization\n",
    "\n",
    ":label:`sec_deferred_init`\n",
    "\n",
    "\n",
    "So far, it might seem that we got away\n",
    "with being sloppy in setting up our networks.\n",
    "Specifically, we did the following unintuitive things,\n",
    "which not might seem like they should work:\n",
    "\n",
    "* We defined the network architectures \n",
    "  without specifying the input dimensionality.\n",
    "* We added layers without specifying\n",
    "  the output dimension of the previous layer.\n",
    "* We even \"initialized\" these parameters \n",
    "  before providing enough information to determine\n",
    "  how many parameters our models should contain.\n",
    "\n",
    "You might be surprised that our code runs at all.\n",
    "After all, there is no way MXNet \n",
    "could tell what the input dimensionality of a network would be.\n",
    "The trick here is that MXNet *defers initialization*,\n",
    "waiting until the first time we pass data through the model,\n",
    "to infer the sizes of each layer *on the fly*.\n",
    "\n",
    "\n",
    "Later on, when working with convolutional neural networks\n",
    "this technique will become even more convenient,\n",
    "since the input dimensionality \n",
    "(i.e., the resolution of an image) \n",
    "will affect the dimensionality \n",
    "of each subsequent layer. \n",
    "Hence, the ability to set parameters \n",
    "without the need to know,\n",
    "at the time of writing the code, \n",
    "what the dimensionality is \n",
    "can greatly simplify the task of specifying \n",
    "and subsequently modifying our models. \n",
    "Next, we go deeper into the mechanics of initialization.\n",
    "\n",
    "\n",
    "## Instantiating a Network\n",
    "\n",
    "To begin, let us instantiate an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "def getnet():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(256, activation='relu'))\n",
    "    net.add(nn.Dense(10))\n",
    "    return net\n",
    "\n",
    "net = getnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the network cannot possibly know\n",
    "the dimensions of the input layer's weights\n",
    "because the input dimension remains unknown.\n",
    "Consequently MXNet has not yet initialized any parameters.\n",
    "We confirm by attempting to access the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Block.collect_params of Sequential(\n",
      "  (0): Dense(-1 -> 256, Activation(relu))\n",
      "  (1): Dense(-1 -> 10, linear)\n",
      ")>\n",
      "sequential0_ (\n",
      "  Parameter dense0_weight (shape=(256, -1), dtype=float32)\n",
      "  Parameter dense0_bias (shape=(256,), dtype=float32)\n",
      "  Parameter dense1_weight (shape=(10, -1), dtype=float32)\n",
      "  Parameter dense1_bias (shape=(10,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net.collect_params)\n",
    "print(net.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the Parameter objects exist,\n",
    "the input dimension to each layer to listed as `-1`.\n",
    "MXNet uses the special value `-1` to indicate\n",
    "that the parameters dimension remains unknown.\n",
    "At this point attempts to access `net[0].weight.data()`\n",
    "would trigger a runtime error stating that the network\n",
    "must be initialized before the parameters can be accessed.\n",
    "Now let us see what happens when we attempt to initialze\n",
    "parameters via the `initialize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential0_ (\n",
       "  Parameter dense0_weight (shape=(256, -1), dtype=float32)\n",
       "  Parameter dense0_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense1_weight (shape=(10, -1), dtype=float32)\n",
       "  Parameter dense1_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, nothing has changed. \n",
    "When input dimensions are known, \n",
    "calls to initialize do not truly initalize the parameters.\n",
    "Instead, this call registers to MXNet that we wish \n",
    "(and optionally, according to which distribution)\n",
    "to initialize the parameters. \n",
    "Only once we pass data through the network\n",
    "will MXNet finally initialize parameters \n",
    "and we will see a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential0_ (\n",
       "  Parameter dense0_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense0_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense1_weight (shape=(10, 256), dtype=float32)\n",
       "  Parameter dense1_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.uniform(size=(2, 20))\n",
    "net(x)  # Forward computation\n",
    "\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as we knew the input dimensionality, \n",
    "$\\mathbf{x} \\in \\mathbb{R}^{20}$ \n",
    "MXNet can identify the shape of the first layer's weight matrix, \n",
    "i.e., $\\mathbf{W}_1 \\in \\mathbb{R}^{256 \\times 20}$.\n",
    "Having recognized the first layer shape, MXNet proceeds\n",
    "to the second layer, whose dimensionality is $10 \\times 256$\n",
    "and so on through the computational graph\n",
    "until all shapes are known.\n",
    "Note that in this case, \n",
    "only the first layer required deferred initialization,\n",
    "but MXNet initializes sequentially. \n",
    "Once all parameter shapes are known, \n",
    "MXNet can finally initialize the parameters. \n",
    "\n",
    "\n",
    "## Deferred Initialization in Practice\n",
    "\n",
    "Now that we know how it works in theory, \n",
    "let us see when the initialization is actually triggered.\n",
    "In order to do so, we mock up an initializer \n",
    "which does nothing but report a debug message \n",
    "stating when it was invoked and with which parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "22"
    }
   },
   "outputs": [],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def _init_weight(self, name, data):\n",
    "        print('Init', name, data.shape)\n",
    "        # The actual initialization logic is omitted here\n",
    "\n",
    "net = getnet()\n",
    "net.initialize(init=MyInit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, although `MyInit` will print information \n",
    "about the model parameters when it is called, \n",
    "the above `initialize` function does not print \n",
    "any information after it has been executed.  \n",
    "Therefore there is no real initialization parameter \n",
    "when calling the `initialize` function. \n",
    "Next, we define the input and perform a forward calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "25"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init dense2_weight (256, 20)\n",
      "Init dense3_weight (10, 256)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(size=(2, 20))\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, information on the model parameters is printed. \n",
    "When performing a forward calculation based on the input `x`,\n",
    "the system can automatically infer the shape of the weight parameters \n",
    "of all layers based on the shape of the input. \n",
    "Once the system has created these parameters, \n",
    "it calls the `MyInit` instance to initialize them \n",
    "before proceeding to the forward calculation.\n",
    "\n",
    "This initialization will only be called \n",
    "when completing the initial forward calculation. \n",
    "After that, we will not re-initialize \n",
    "when we run the forward calculation `net(x)`, \n",
    "so the output of the `MyInit` instance will not be generated again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the beginning of this section,\n",
    "deferred initialization can be source of confusion.\n",
    "Before the first forward calculation,\n",
    "we were unable to directly manipulate the model parameters,\n",
    "for example, we could not use\n",
    "the `data` and `set_data` functions\n",
    "to get and modify the parameters.\n",
    "Therefore, we often force initialization\n",
    "by sending a sample observation through the network.\n",
    "\n",
    "## Forced Initialization\n",
    "\n",
    "Deferred initialization does not occur \n",
    "if the system knows the shape of all parameters \n",
    "when we call the `initialize` function. \n",
    "This can occur in two cases:\n",
    "\n",
    "* We have already seen some data and we just want to reset the parameters.\n",
    "* We specified all input and output dimensions of the network when defining it.\n",
    "\n",
    "Forced reinitialization works as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init dense2_weight (256, 20)\n",
      "Init dense3_weight (10, 256)\n"
     ]
    }
   ],
   "source": [
    "net.initialize(init=MyInit(), force_reinit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second case requires that we specify \n",
    "all parameters when creating each layer.\n",
    "For instance, for dense layers we must specify `in_units` \n",
    "at the time that the layer is instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init dense4_weight (256, 20)\n",
      "Init dense5_weight (10, 256)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, in_units=20, activation='relu'))\n",
    "net.add(nn.Dense(10, in_units=256))\n",
    "\n",
    "net.initialize(init=MyInit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Deferred initialization can be convenient, allowing Gluon to infer parameter shapes automatically, making it easy to modify architectures and eliminating one common source of errors.\n",
    "* We do not need deferred initialization when we specify all variables explicitly.\n",
    "* We can forcibly re-initialize a network's parameters by invoking initalize with the `force_reinit=True` flag.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. What happens if you specify the input dimensions to the first laye but not to subsequent layers? Do you get immediate initialization?\n",
    "1. What happens if you specify mismatching dimensions?\n",
    "1. What would you need to do if you have input of varying dimensionality? Hint - look at parameter tying.\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/2327)\n",
    "\n",
    "![](../img/qr_deferred-init.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}