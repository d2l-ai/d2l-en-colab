{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n",
    "!pip install ..  # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-Aware Recommender Systems\n",
    "\n",
    "In previous sections, we abstract the recommendation task as a matrix completion problem without considering users' short-term behaviors. In this section, we will introduce a recommendation model that takes  the sequentially-ordered user interaction logs into account.  It is a sequence-aware recommender :cite:`Quadrana.Cremonesi.Jannach.2018` where the input is an ordered and often timestamped list of past user actions.  A number of recent literatures have demonstrated the usefulness of incorporating such information in modeling users' temporal behavioral patterns and discovering their interest drift.  \n",
    "\n",
    "The model we will introduce, Caser :cite:`Sedhain.Menon.Sanner.ea.2015`, short for convolutional sequence embedding recommendation model, adopts convolutional neural networks capture the dynamic pattern influences of users' recent activities. The main component of Caser consists of a horizontal convolutional network and a vertical convolutional network, aiming to uncover the union-level and point-level sequence patterns, respectively.  Point-level pattern indicates the impact of single item in the historical sequence on the target item, while union level pattern implies the influences of several previous actions on the subsequent target. For example, buying both milk and butter together leads to higher probability of buying flour than just buying one of them. Moreover, users' general interests, or long term preferences are also modeled in the last fully-connected layers, resulting in a more comprehensive modeling of user interests. Details of the model are described as follows.\n",
    "\n",
    "## Model Architectures\n",
    "\n",
    "In sequence-aware recommendation system, each user is associated with a sequence of some items from the item set. Let $S^u = (S_1^u, ... S_{|S_u|}^u)$ denotes the ordered sequence. The goal of Caser is to recommend item by considering user general tastes as well as short-term intention. Suppose we take the previous $L$ items into consideration, an embedding matrix that represents the former interactions for timestep $t$ can be constructed:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}^{(u, t)} = [ \\mathbf{q}_{S_{t-L}^u} , ..., \\mathbf{q}_{S_{t-2}^u}, \\mathbf{q}_{S_{t-1}^u} ]^\\top,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{Q} \\in \\mathbb{R}^{n \\times k}$ represents item embeddings and $\\mathbf{q}_i$ denotes the $i^\\mathrm{th}$ row. $\\mathbf{E}^{(u, t)} \\in \\mathbb{R}^{L \\times k}$ can be used to infer the transient interest of user $u$ at time-step $t$. We can view the input matrix $\\mathbf{E}^{(u, t)}$ as an image which is the input of the subsequent two convolutional components.\n",
    "\n",
    "The horizontal convolutional layer has $d$ horizontal filters $\\mathbf{F}^j \\in \\mathbb{R}^{h \\times k}, 1 \\leq j \\leq d, h = \\{1, ..., L\\}$, and the vertical convolutional layer has $d'$ vertical filters $\\mathbf{G}^j \\in \\mathbb{R}^{ L \\times 1}, 1 \\leq j \\leq d'$. After a series of convolutional and pool operations, we get the two outputs:\n",
    "\n",
    "$$\n",
    "\\mathbf{o} = \\text{HConv}(\\mathbf{E}^{(u, t)}, \\mathbf{F}) \\\\\n",
    "\\mathbf{o}'= \\text{VConv}(\\mathbf{E}^{(u, t)}, \\mathbf{G}) ,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{o} \\in \\mathbb{R}^d$ is the output of horizontal convolutional network and $\\mathbf{o}' \\in \\mathbb{R}^{kd'}$ is the output of vertical convolutional network. For simplicity, we omit the details of convolution and pool operations. They are concatenated and fed into a fully-connected neural network layer to get more high-level representations.\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = \\phi(\\mathbf{W}[\\mathbf{o}, \\mathbf{o}']^\\top + \\mathbf{b}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W} \\in \\mathbb{R}^{k \\times (d + kd')}$ is the weight matrix and $\\mathbf{b} \\in \\mathbb{R}^k$ is the bias. The learned vector $\\mathbf{z} \\in \\mathbb{R}^k$ is the representation of user's short-term intent.\n",
    "\n",
    "At last, the prediction function combines users' short-term and general taste together, which is defined as:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{uit} = \\mathbf{v}_i \\cdot [\\mathbf{z}, \\mathbf{p}_u]^\\top + \\mathbf{b}'_i,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{V} \\in \\mathbb{R}^{n \\times 2k}$ is another item embedding matrix. $\\mathbf{b}' \\in \\mathbb{R}^n$ is the item specific bias.  $\\mathbf{P} \\in \\mathbb{R}^{m \\times k}$ is the user embedding matrix for users' general tastes. $\\mathbf{p}_u \\in \\mathbb{R}^{ k}$ is the $u^\\mathrm{th}$ row of $P$ and $\\mathbf{v}_i \\in \\mathbb{R}^{2k}$ is the $i^\\mathrm{th}$ row of $\\mathbf{V}$.\n",
    "\n",
    "The model can be learned with BPR or Hinge loss. The architecture of Caser is shown below:\n",
    "\n",
    "![Illustration of the Caser Model](../img/rec-caser.svg)\n",
    "\n",
    "We first import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "import mxnet as mx\n",
    "import random\n",
    "import sys\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "The following code implements the Caser model. It consists of a vertical convolutional layer, a horizontal convolutional layer, and a full-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "class Caser(nn.Block):\n",
    "    def __init__(self, num_factors, num_users, num_items, L=5, d=16,\n",
    "                 d_prime=4, drop_ratio=0.05, **kwargs):\n",
    "        super(Caser, self).__init__(**kwargs)\n",
    "        self.P = nn.Embedding(num_users, num_factors)\n",
    "        self.Q = nn.Embedding(num_items, num_factors)\n",
    "        self.d_prime, self.d = d_prime, d\n",
    "        # Vertical convolution layer\n",
    "        self.conv_v = nn.Conv2D(d_prime, (L, 1), in_channels=1)\n",
    "        # Horizontal convolution layer\n",
    "        h = [i + 1 for i in range(L)]\n",
    "        self.conv_h, self.max_pool = nn.Sequential(), nn.Sequential()\n",
    "        for i in h:\n",
    "            self.conv_h.add(nn.Conv2D(d, (i, num_factors), in_channels=1))\n",
    "            self.max_pool.add(nn.MaxPool1D(L - i + 1))\n",
    "        # Fully-connected layer\n",
    "        self.fc1_dim_v, self.fc1_dim_h = d_prime * num_factors, d * len(h)\n",
    "        self.fc = nn.Dense(in_units=d_prime * num_factors + d * L,\n",
    "                           activation='relu', units=num_factors)\n",
    "        self.Q_prime = nn.Embedding(num_items, num_factors * 2)\n",
    "        self.b = nn.Embedding(num_items, 1)\n",
    "        self.dropout = nn.Dropout(drop_ratio)\n",
    "        \n",
    "    def forward(self, user_id, seq, item_id):\n",
    "        item_embs = np.expand_dims(self.Q(seq), 1)\n",
    "        user_emb = self.P(user_id)\n",
    "        out, out_h, out_v, out_hs = None, None, None, []\n",
    "        if self.d_prime:\n",
    "            out_v = self.conv_v(item_embs)\n",
    "            out_v = out_v.reshape(out_v.shape[0], self.fc1_dim_v)\n",
    "        if self.d:\n",
    "            for conv, maxp in zip(self.conv_h, self.max_pool):\n",
    "                conv_out = np.squeeze(npx.relu(conv(item_embs)), axis=3)\n",
    "                t = maxp(conv_out)\n",
    "                pool_out = np.squeeze(t, axis=2)\n",
    "                out_hs.append(pool_out)\n",
    "            out_h = np.concatenate(out_hs, axis=1)\n",
    "        out = np.concatenate([out_v, out_h], axis=1)\n",
    "        z = self.fc(self.dropout(out))\n",
    "        x = np.concatenate([z, user_emb], axis=1)\n",
    "        q_prime_i = np.squeeze(self.Q_prime(item_id))\n",
    "        b = np.squeeze(self.b(item_id))\n",
    "        res = (x * q_prime_i).sum(1) + b\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Dataset with Negative Sampling\n",
    "To process the sequential interaction data, we need to reimplement the Dataset class. The following code creates a new dataset class named `SeqDataset`. In each sample, it outputs the user identity, her previous $L$ interacted items as a sequence and the next item she interacts as the target. The following figure demonstrates the data loading process for one user. Suppose that this user liked 8 movies, we organize these eight movies in chronological order. The latest movie is left out as the test item. For the remaining seven movies, we can get three training samples, with each sample containing a sequence of five ($L=5$) movies and its subsequent item as the target item. Negative samples are also included in the Customized dataset. \n",
    "\n",
    "![Illustration of the data generation process](../img/rec-seq-data.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "class SeqDataset(gluon.data.Dataset):\n",
    "    def __init__(self, user_ids, item_ids, L, num_users, num_items, \n",
    "                 candidates):\n",
    "        user_ids, item_ids = np.array(user_ids), np.array(item_ids)\n",
    "        sort_idx = np.array(sorted(range(len(user_ids)),\n",
    "                                   key=lambda k: user_ids[k]))\n",
    "        u_ids, i_ids = user_ids[sort_idx], item_ids[sort_idx]\n",
    "        temp, u_ids, self.cand = {}, u_ids.asnumpy(), candidates\n",
    "        self.all_items = set([i for i in range(num_items)])\n",
    "        [temp.setdefault(u_ids[i], []).append(i) for i, _ in enumerate(u_ids)]\n",
    "        temp = sorted(temp.items(), key=lambda x: x[0])\n",
    "        u_ids = np.array([i[0] for i in temp])\n",
    "        idx = np.array([i[1][0] for i in temp])\n",
    "        self.ns = ns = int(sum([c - L if c >= L + 1 else 1 for c\n",
    "                                in np.array([len(i[1]) for i in temp])]))\n",
    "        self.seq_items = np.zeros((ns, L))\n",
    "        self.seq_users = np.zeros(ns, dtype='int32')\n",
    "        self.seq_tgt = np.zeros((ns, 1))\n",
    "        self.test_seq = np.zeros((num_users, L))\n",
    "        test_users, _uid = np.empty(num_users), None\n",
    "        for i, (uid, i_seq) in enumerate(self._seq(u_ids, i_ids, idx, L + 1)):\n",
    "            if uid != _uid:\n",
    "                self.test_seq[uid][:] = i_seq[-L:]\n",
    "                test_users[uid], _uid = uid, uid\n",
    "            self.seq_tgt[i][:] = i_seq[-1:]\n",
    "            self.seq_items[i][:], self.seq_users[i] = i_seq[:L], uid\n",
    "            \n",
    "    def _win(self, tensor, window_size, step_size=1):\n",
    "        if len(tensor) - window_size >= 0:\n",
    "            for i in range(len(tensor), 0, - step_size):\n",
    "                if i - window_size >= 0:\n",
    "                    yield tensor[i - window_size:i]\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            yield tensor\n",
    "            \n",
    "    def _seq(self, u_ids, i_ids, idx, max_len):\n",
    "        for i in range(len(idx)):\n",
    "            stop_idx = None if i >= len(idx) - 1 else int(idx[i + 1])\n",
    "            for s in self._win(i_ids[int(idx[i]):stop_idx], max_len):\n",
    "                yield (int(u_ids[i]), s)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.ns\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        neg = list(self.all_items - set(self.cand[int(self.seq_users[i])]))\n",
    "        idx = random.randint(0, len(neg) - 1)\n",
    "        return self.seq_users[i], self.seq_items[i], self.seq_tgt[i], neg[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MovieLens 100K dataset\n",
    "\n",
    "Afterwards, we read and split the MovieLens 100K dataset in sequence-aware mode and load the training data with sequential dataloader implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_NUM, L, batch_size = 1, 3, 4096\n",
    "df, num_users, num_items = d2l.read_data_ml100k()\n",
    "train_data, test_data = d2l.split_data_ml100k(df, num_users, num_items,\n",
    "                                              'seq-aware')\n",
    "users_train, items_train, ratings_train, candidates = d2l.load_data_ml100k(\n",
    "    train_data, num_users, num_items, feedback=\"implicit\")\n",
    "users_test, items_test, ratings_test, test_iter = d2l.load_data_ml100k(\n",
    "    test_data, num_users, num_items, feedback=\"implicit\")\n",
    "train_seq_data = SeqDataset(users_train, items_train, L, num_users,\n",
    "                            num_items, candidates)\n",
    "num_workers = 0 if sys.platform.startswith(\"win\") else 4\n",
    "train_iter = gluon.data.DataLoader(train_seq_data, batch_size, True,\n",
    "                                   last_batch=\"rollover\",\n",
    "                                   num_workers=num_workers)\n",
    "test_seq_iter = train_seq_data.test_seq\n",
    "train_seq_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data structure is shown above. The first element is the user identity, the next list indicates the last five items this user liked, and the last element is the item this user liked after the five items.\n",
    "\n",
    "## Train the Model\n",
    "Now, let's train the model. We use the same setting as NeuMF, including learning rate, optimizer, and $k$, in the last section so that the results are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "ctx = d2l.try_all_gpus()\n",
    "net = Caser(10, num_users, num_items, L)\n",
    "net.initialize(ctx=ctx, force_reinit=True, init=mx.init.Normal(0.01))\n",
    "lr, num_epochs, wd, optimizer = 0.04, 8, 1e-5, 'adam'\n",
    "loss = d2l.BPRLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer,\n",
    "                        {\"learning_rate\": lr, 'wd': wd})\n",
    "\n",
    "d2l.train_ranking(net, train_iter, test_iter, loss, trainer, test_seq_iter,\n",
    "                  num_users, num_items, num_epochs, ctx, d2l.evaluate_ranking,\n",
    "                  candidates, eval_step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* Inferring a user's short-term and long-term interests can make prediction of the next item that she preferred more effectively.\n",
    "* Convolutional neural networks can be utilized to capture users' short-term interests from sequential interactions.\n",
    "\n",
    "## Exercises\n",
    "* Conduct an ablation study by removing one of the horizontal and vertical convolutional networks, which component is the more important ?\n",
    "* Vary the hyper-parameter $L$. Does longer historical interactions bring higher accuracy?\n",
    "* Apart from the sequence-aware recommendation task we introduced above, there is another type of sequence-aware recommendation task called session-based recommendation :cite:`Hidasi.Karatzoglou.Baltrunas.ea.2015`. Can you explain the differences between these two tasks?\n",
    "\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/5165)\n",
    "\n",
    "![](http://d2l.ai/_images/qr_seqrec.svg)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}