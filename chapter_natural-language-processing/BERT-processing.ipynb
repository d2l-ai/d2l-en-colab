{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/d2l-ai/d2l-en # installing d2l\n",
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, np, npx\n",
    "from mxnet.contrib import text\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip',\n",
    "    '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "# Saved in the d2l package for later use\n",
    "d2l.DATA_HUB['wikitext-103'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip',\n",
    "    '0aec09a7537b58d4bb65362fee27650eeaba625a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        raw = f.readlines()\n",
    "    data = [line.strip().lower().split(' . ')\n",
    "            for line in raw if len(line.split(' . '))>=2]\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def get_next_sentence(sentence, next_sentence, all_documents):\n",
    "    if random.random() < 0.5:\n",
    "        tokens_a = sentence\n",
    "        tokens_b = next_sentence\n",
    "        is_next = True\n",
    "    else:\n",
    "        random_sentence = random.choice(random.choice(all_documents))\n",
    "        tokens_a = sentence\n",
    "        tokens_b = random_sentence\n",
    "        is_next = False\n",
    "    return tokens_a, tokens_b, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def get_tokens_and_segment(tokens_a, tokens_b):\n",
    "    tokens = [] \n",
    "    segment_ids = [] \n",
    "\n",
    "    tokens.append('[CLS]')\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append('[SEP]')\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    tokens.append('[SEP]')\n",
    "    segment_ids.append(1)\n",
    "    \n",
    "    return tokens, segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def create_next_sentence(document, all_documents, vocab, max_length):\n",
    "    instances = []\n",
    "    for i in range(len(document)-1):\n",
    "        tokens_a, tokens_b, is_next = get_next_sentence(document[i], document[i+1], all_documents)\n",
    "        \n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_length:\n",
    "             continue\n",
    "        tokens, segment_ids = get_tokens_and_segment(tokens_a, tokens_b)\n",
    "        instances.append((tokens, segment_ids, is_next))\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def choice_mask_tokens(tokens, cand_indexes, num_to_predict, vocab):\n",
    "    output_tokens = list(tokens)\n",
    "    masked_lms = []\n",
    "    random.shuffle(cand_indexes)\n",
    "    for index_set in cand_indexes:\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "            continue\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if random.random() < 0.8:\n",
    "                masked_token = '[MASK]'\n",
    "            else:\n",
    "                if random.random() < 0.5:\n",
    "                    masked_token = tokens[index]\n",
    "                else:\n",
    "                    masked_token = random.randint(0, len(vocab) - 1)\n",
    "\n",
    "            output_tokens[index] = masked_token\n",
    "            masked_lms.append((index, tokens[index]))\n",
    "    return output_tokens, masked_lms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def create_masked_lm(tokens, vocab):\n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token in ['[CLS]', '[SEP]']:\n",
    "            continue\n",
    "        cand_indexes.append([i])\n",
    "        \n",
    "    num_to_predict = max(1, int(round(len(tokens) * 0.15)))\n",
    "    \n",
    "    output_tokens, masked_lms = choice_mask_tokens(tokens, cand_indexes,\n",
    "                                                   num_to_predict, vocab)\n",
    "            \n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x[0])\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p[0])\n",
    "        masked_lm_labels.append(p[1])\n",
    "        \n",
    "    return vocab[output_tokens], masked_lm_positions, vocab[masked_lm_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def convert_numpy(instances, max_length):\n",
    "    input_ids, segment_ids, masked_lm_positions, masked_lm_ids = [], [], [], []\n",
    "    masked_lm_weights, next_sentence_labels, valid_lengths = [], [], []\n",
    "    for instance in instances:\n",
    "        input_id = instance[0] + [0] * (max_length - len(instance[0]))\n",
    "        segment_id = instance[3] + [0] * (max_length - len(instance[3]))\n",
    "        masked_lm_position = instance[1] + [0] * (20 - len(instance[1]))\n",
    "        masked_lm_id = instance[2] + [0] * (20 - len(instance[2]))\n",
    "        masked_lm_weight = [1.0] * len(instance[2]) + [0.0] * (20 - len(instance[1]))\n",
    "        next_sentence_label = instance[4]\n",
    "        valid_length = len(instance[0])\n",
    "\n",
    "        input_ids.append(np.array(input_id, dtype='int32'))\n",
    "        segment_ids.append(np.array(segment_id, dtype='int32'))\n",
    "        masked_lm_positions.append(np.array(masked_lm_position, dtype='int32'))\n",
    "        masked_lm_ids.append(np.array(masked_lm_id, dtype='int32'))\n",
    "        masked_lm_weights.append(np.array(masked_lm_weight, dtype='float32'))\n",
    "        next_sentence_labels.append(np.array(next_sentence_label))\n",
    "        valid_lengths.append(np.array(valid_length))\n",
    "    return input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights,\\\n",
    "           next_sentence_labels, segment_ids, valid_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def create_training_instances(train_data, vocab, max_length):\n",
    "    instances = []\n",
    "    for i, document in enumerate(train_data):\n",
    "        instances.extend(create_next_sentence(document, train_data, vocab, max_length))\n",
    "    instances = [(create_masked_lm(tokens, vocab) + (segment_ids, is_random_next))\n",
    "                 for (tokens, segment_ids, is_random_next) in instances]\n",
    "    input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights,\\\n",
    "           next_sentence_labels, segment_ids, valid_lengths = convert_numpy(instances, max_length)\n",
    "    return input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights,\\\n",
    "           next_sentence_labels, segment_ids, valid_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class WikiDataset(gluon.data.Dataset):\n",
    "    def __init__(self, dataset, max_length = 128):\n",
    "        train_tokens = [d2l.tokenize(row, token='word') for row in dataset]\n",
    "        \n",
    "        text_list=[]\n",
    "        [text_list.extend(row) for row in train_tokens]\n",
    "        self.vocab = d2l.Vocab(text_list, min_freq=5, \n",
    "                               reserved_tokens=['[MASK]', '[CLS]', '[SEP]'])\n",
    "        self.input_ids, self.masked_lm_ids, self.masked_lm_positions,\\\n",
    "        self.masked_lm_weights, self.next_sentence_labels, self.segment_ids,\\\n",
    "        self.valid_lengths = create_training_instances(train_tokens, self.vocab, max_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.masked_lm_ids[idx], self.masked_lm_positions[idx], self.masked_lm_weights[idx],\\\n",
    "           self.next_sentence_labels[idx], self.segment_ids[idx], self.valid_lengths[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def load_data_wiki(batch_size, data_set = 'wikitext-2', num_steps=128):\n",
    "    data_dir = d2l.download_extract(data_set, data_set)\n",
    "    train_data = read_wiki(data_dir)\n",
    "    train_set = WikiDataset(train_data, num_steps)\n",
    "    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_iter, vocab = load_data_wiki(batch_size, 'wikitext-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 128) (512, 20) (512, 20) (512, 20) (512,) (512, 128) (512,)\n"
     ]
    }
   ],
   "source": [
    "for _, data_batch in enumerate(train_iter):\n",
    "    (input_id, masked_id, masked_position, masked_weight, \\\n",
    "     next_sentence_label, segment_id, valid_length) = data_batch\n",
    "    print(input_id.shape, masked_id.shape, masked_position.shape, masked_weight.shape,\\\n",
    "          next_sentence_label.shape, segment_id.shape, valid_length.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(len(vocab), embed_size=128, hidden_size=256, \n",
    "                    num_heads=2, num_layers=2, dropout=0.2)\n",
    "ctx = d2l.try_all_gpus()\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "nsp_loss = mx.gluon.loss.SoftmaxCELoss()\n",
    "mlm_loss = mx.gluon.loss.SoftmaxCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def _get_batch_bert(batch, ctx):\n",
    "    (input_id, masked_id, masked_position, masked_weight, \\\n",
    "     next_sentence_label, segment_id, valid_length) = batch\n",
    "    \n",
    "    return (gluon.utils.split_and_load(input_id, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(masked_id, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(masked_position, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(masked_weight, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(next_sentence_label, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(segment_id, ctx, even_split=False),\n",
    "            gluon.utils.split_and_load(valid_length.astype('float32'), ctx, even_split=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def batch_loss_bert(net, nsp_loss, mlm_loss, input_id, masked_id, masked_position,\n",
    "                    masked_weight, next_sentence_label, segment_id, valid_length, vocab_size):\n",
    "    ls = []\n",
    "    ls_mlm = []\n",
    "    ls_nsp = []\n",
    "    for i_id, m_id, m_pos, m_w, nsl, s_i, v_l in zip(input_id, masked_id, masked_position, masked_weight,\\\n",
    "                                                      next_sentence_label, segment_id, valid_length):\n",
    "        num_masks = m_w.sum() + 1e-8\n",
    "        _, classified, decoded = net(i_id, s_i, v_l.reshape(-1),m_pos)\n",
    "        l_mlm = mlm_loss(decoded.reshape((-1, vocab_size)),m_id.reshape(-1), m_w.reshape((-1, 1)))\n",
    "        l_mlm = l_mlm.sum() / num_masks\n",
    "        l_nsp = nsp_loss(classified, nsl)\n",
    "        l_nsp = l_nsp.mean()\n",
    "        l = l_mlm + l_nsp\n",
    "        ls.append(l)\n",
    "        ls_mlm.append(l_mlm)\n",
    "        ls_nsp.append(l_nsp)\n",
    "        npx.waitall()\n",
    "    return ls, ls_mlm, ls_nsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def train_bert(data_eval, net, nsp_loss, mlm_loss, vocab_size, ctx, log_interval, max_step):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam')\n",
    "    step_num = 0\n",
    "    while step_num < max_step:\n",
    "        eval_begin_time = time.time()\n",
    "        begin_time = time.time()\n",
    "        \n",
    "        running_mlm_loss = running_nsp_loss = 0\n",
    "        total_mlm_loss = total_nsp_loss = 0\n",
    "        running_num_tks = 0\n",
    "        for _, data_batch in enumerate(data_eval):\n",
    "            (input_id, masked_id, masked_position, masked_weight, \\\n",
    "             next_sentence_label, segment_id, valid_length) = _get_batch_bert(data_batch, ctx)\n",
    "            \n",
    "            step_num += 1\n",
    "            with autograd.record():\n",
    "                ls, ls_mlm, ls_nsp = batch_loss_bert(net, nsp_loss, mlm_loss, input_id, masked_id, masked_position, masked_weight, next_sentence_label, segment_id, valid_length, vocab_size)\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "            \n",
    "            trainer.step(1)\n",
    "            \n",
    "            running_mlm_loss += sum([l for l in ls_mlm])\n",
    "            running_nsp_loss += sum([l for l in ls_nsp])\n",
    "\n",
    "            if (step_num + 1) % (log_interval) == 0:\n",
    "                total_mlm_loss += running_mlm_loss\n",
    "                total_nsp_loss += running_nsp_loss\n",
    "                begin_time = time.time()\n",
    "                running_mlm_loss = running_nsp_loss = 0\n",
    "\n",
    "        eval_end_time = time.time()\n",
    "        if running_mlm_loss != 0:\n",
    "            total_mlm_loss += running_mlm_loss\n",
    "            total_nsp_loss += running_nsp_loss\n",
    "        total_mlm_loss /= step_num\n",
    "        total_nsp_loss /= step_num\n",
    "        print('Eval mlm_loss={:.3f}\\tnsp_loss={:.3f}\\t'\n",
    "                     .format(float(total_mlm_loss),\n",
    "                             float(total_nsp_loss)))\n",
    "        print('Eval cost={:.1f}s'.format(eval_end_time - eval_begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval mlm_loss=14.653\tnsp_loss=1.445\t\n",
      "Eval cost=8.0s\n"
     ]
    }
   ],
   "source": [
    "train_bert(train_iter, net, nsp_loss, mlm_loss, len(vocab), ctx, 20, 1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}