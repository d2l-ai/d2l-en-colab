{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers\n",
    "\n",
    "One of factors behind deep learnings success\n",
    "is the availability of a wide range of layers\n",
    "that can be composed in creative ways\n",
    "to design architectures suitable\n",
    "for a wide variety of tasks.\n",
    "For instance, researchers have invented layers\n",
    "specifically for handling images, text,\n",
    "looping over sequential data,\n",
    "performing dynamic programming, etc.\n",
    "Sooner or later you will encounter (or invent)\n",
    "a layer that does not exist yet in Gluon,\n",
    "In these cases, you must build a custom layer.\n",
    "In this section, we show you how.\n",
    "\n",
    "## Layers without Parameters\n",
    "\n",
    "To start, we construct a custom layer (a Block) \n",
    "that does not have any parameters of its own. \n",
    "This should look familiar if you recall our \n",
    "introduction to Gluon's `Block` in :numref:`sec_model_construction`. \n",
    "The following `CenteredLayer` class simply\n",
    "subtracts the mean from its input. \n",
    "To build it, we simply need to inherit \n",
    "from the Block class and implement the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "class CenteredLayer(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CenteredLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that our layer works as intended by feeding some data through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(np.array([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now incorporate our layer as a component\n",
    "in constructing more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(128), CenteredLayer())\n",
    "net.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an extra sanity check, we can send random data \n",
    "through the network and check that the mean is in fact 0.\n",
    "Because we are dealing with floating point numbers, \n",
    "we may still see a *very* small nonzero number\n",
    "due to quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3.783498e-10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = net(np.random.uniform(size=(4, 8)))\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers with Parameters\n",
    "\n",
    "Now that we know how to define simple layers\n",
    "let us move on to defining layers with parameters\n",
    "that can be adjusted through training. \n",
    "To automate some of the routine work\n",
    "the `Parameter` class and the `ParameterDict` dictionary \n",
    "provide some basic housekeeping functionality.\n",
    "In particular, they govern access, initialization, \n",
    "sharing, saving and loading model parameters. \n",
    "This way, among other benefits, we will not need to write\n",
    "custom serialization routines for every custom layer.\n",
    "\n",
    "The `Block` class contains a `params` variable\n",
    "of the `ParameterDict` type. \n",
    "This dictionary maps strings representing parameter names\n",
    "to model parameters (of the `Parameter` type). \n",
    "The `ParameterDict` also supplied a `get` function\n",
    "that makes it easy to generate a new parameter\n",
    "with a specified name and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  Parameter param2 (shape=(2, 3), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = gluon.ParameterDict()\n",
    "params.get('param2', shape=(2, 3))\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the basic ingredients that we need\n",
    "to implement our own version of Gluon's `Dense` layer. \n",
    "Recall that this layer requires two parameters,\n",
    "one to represent the weight and another for the bias. \n",
    "In this implementation, we bake in the ReLU activation as a default.\n",
    "In the `__init__` function, `in_units` and `units`\n",
    "denote the number of inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [],
   "source": [
    "class MyDense(nn.Block):\n",
    "    # units: the number of outputs in this layer; in_units: the number of\n",
    "    # inputs in this layer\n",
    "    def __init__(self, units, in_units, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=(in_units, units))\n",
    "        self.bias = self.params.get('bias', shape=(units,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = np.dot(x, self.weight.data()) + self.bias.data()\n",
    "        return npx.relu(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming our parameters allows us to access them \n",
    "by name through dictionary lookup later.\n",
    "Generally, you will want to give your variables\n",
    "simple names that make their purpose clear.\n",
    "Next, we instantiate the `MyDense` class \n",
    "and access its model parameters.\n",
    "Note that the Block's name is automatically\n",
    "prepended to each Parameter's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mydense0_ (\n",
       "  Parameter mydense0_weight (shape=(5, 3), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mydense0_bias (shape=(3,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = MyDense(units=3, in_units=5)\n",
    "dense.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly carry out forward calculations using custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.01633355, 0.        ],\n",
       "       [0.        , 0.01581812, 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.initialize()\n",
    "dense(np.random.uniform(size=(2, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also construct models using custom layers.\n",
    "Once we have that we can use it just like the built-in dense layer.\n",
    "The only exception is that in our case,\n",
    "shape inference is not automatic. \n",
    "If you are interested in these bells and whisteles,\n",
    "please consult the [MXNet documentation](http://www.mxnet.io)\n",
    "for details on how to implement shape inference in custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06508517],\n",
       "       [0.0615553 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(MyDense(8, in_units=64),\n",
    "        MyDense(1, in_units=8))\n",
    "net.initialize()\n",
    "net(np.random.uniform(size=(2, 64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* We can design custom layers via the Block class. This allows us to define flexible new layers that behave differently from any existing layers in the library.\n",
    "* Once defined, custom layers can be invoked in arbitrary contexts and architectures.\n",
    "* Blocks can have local parameters, which are stored as a `ParameterDict` object in each Blovk's `params` attribute.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Design a layer that learns an affine transform of the data.\n",
    "1. Design a layer that takes an input and computes a tensor reduction, \n",
    "   i.e., it returns $y_k = \\sum_{i, j} W_{ijk} x_i x_j$.\n",
    "1. Design a layer that returns the leading half of the Fourier coefficients of the data. Hint: look up the `fft` function in MXNet.\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/2328)\n",
    "\n",
    "![](../img/qr_custom-layer.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}