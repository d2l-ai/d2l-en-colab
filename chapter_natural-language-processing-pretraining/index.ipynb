{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Pretraining\n",
    "\n",
    ":label:`chap_nlp_pretrain`\n",
    "\n",
    "\n",
    "\n",
    "Humans need to communicate.\n",
    "Out of this basic need of the human condition, a vast amount of written text has been generated on an everyday basis.\n",
    "Given rich text in social media, chat apps, emails, product reviews, news articles,  research papers, and books, it becomes vital to enable computers to understand them to offer assistance or make decisions based on human languages.\n",
    "\n",
    "Natural language processing (NLP) studies interactions between computers and humans using natural languages.\n",
    "In practice, it is very common to use NLP techniques to process and analyze text (human natural language) data, such as language models in :numref:`sec_language_model` and machine translation models in :numref:`sec_machine_translation`.\n",
    "\n",
    "To understand text, we can begin with its representation,\n",
    "such as treating each word or subword as an individual text token.\n",
    "As we will see in this chapter,\n",
    "the representation of each token can be pretrained on a large corpus,\n",
    "using word2vec, GloVe, or subword embedding models.\n",
    "After pretraining, representation of each token can be a vector,\n",
    "and it remains the same no matter what the context is.\n",
    "For instance, the vector representation of \"bank\" is the same\n",
    "in both \n",
    "\"go to the bank to deposit some money\"\n",
    "and \n",
    "\"go to the bank to sit down\".\n",
    "Many more recent pretraining models can incorporate context information.\n",
    "Among them is BERT, a much deeper model based on the Transformer encoder.\n",
    "In this chapter, we will focus on how to pretrain such representations for text,\n",
    "as highlighted in :numref:`fig_nlp-map-pretrain`.\n",
    "\n",
    "![Pretrained text representation can be fed to various deep learning architectures for different downstream NLP tasks. This chapter focuses on the upstream text representation pretraining.](../img/nlp-map-pretrain.svg)\n",
    "\n",
    ":label:`fig_nlp-map-pretrain`\n",
    "\n",
    "\n",
    "As shown in :numref:`fig_nlp-map-pretrain`,\n",
    "the pretrained text representation can be fed to\n",
    "a variety of deep learning architectures for different downstream NLP tasks.\n",
    "We will cover them in :numref:`chap_nlp_app`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [word2vec](word2vec.ipynb)\n",
    " - [approx-training](approx-training.ipynb)\n",
    " - [word2vec-dataset](word2vec-dataset.ipynb)\n",
    " - [word2vec-gluon](word2vec-gluon.ipynb)\n",
    " - [glove](glove.ipynb)\n",
    " - [subword-embedding](subword-embedding.ipynb)\n",
    " - [similarity-analogy](similarity-analogy.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}