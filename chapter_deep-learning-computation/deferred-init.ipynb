{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U mxnet-cu101mkl==1.6.0  # updating mxnet to at least v1.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Deferred Initialization\n",
    ":label:`sec_deferred_init`\n",
    "\n",
    "So far, it might seem that we got away\n",
    "with being sloppy in setting up our networks.\n",
    "Specifically, we did the following unintuitive things,\n",
    "which might not seem like they should work:\n",
    "\n",
    "* We defined the network architectures\n",
    "  without specifying the input dimensionality.\n",
    "* We added layers without specifying\n",
    "  the output dimension of the previous layer.\n",
    "* We even \"initialized\" these parameters\n",
    "  before providing enough information to determine\n",
    "  how many parameters our models should contain.\n",
    "\n",
    "You might be surprised that our code runs at all.\n",
    "After all, there is no way MXNet and TensorFlow\n",
    "could tell what the input dimensionality of a network would be.\n",
    "The trick here is that both frameworks *defer initialization*,\n",
    "waiting until the first time we pass data through the model,\n",
    "to infer the sizes of each layer *on the fly*.\n",
    "\n",
    "\n",
    "Later on, when working with convolutional neural networks,\n",
    "this technique will become even more convenient\n",
    "since the input dimensionality\n",
    "(i.e., the resolution of an image)\n",
    "will affect the dimensionality\n",
    "of each subsequent layer.\n",
    "Hence, the ability to set parameters\n",
    "without the need to know,\n",
    "at the time of writing the code,\n",
    "what the dimensionality is\n",
    "can greatly simplify the task of specifying\n",
    "and subsequently modifying our models.\n",
    "Next, we go deeper into the mechanics of initialization.\n",
    "\n",
    "\n",
    "## Instantiating a Network\n",
    "\n",
    "To begin, let us instantiate an MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 1,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "from mxnet import init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "def getnet():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(256, activation='relu'))\n",
    "    net.add(nn.Dense(10))\n",
    "    return net\n",
    "\n",
    "net = getnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "At this point, the network cannot possibly know\n",
    "the dimensions of the input layer's weights\n",
    "because the input dimension remains unknown.\n",
    "Consequently the framework has not yet initialized any parameters.\n",
    "We confirm by attempting to access the parameters below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 4,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Block.collect_params of Sequential(\n",
      "  (0): Dense(-1 -> 256, Activation(relu))\n",
      "  (1): Dense(-1 -> 10, linear)\n",
      ")>\n",
      "sequential0_ (\n",
      "  Parameter dense0_weight (shape=(256, -1), dtype=float32)\n",
      "  Parameter dense0_bias (shape=(256,), dtype=float32)\n",
      "  Parameter dense1_weight (shape=(10, -1), dtype=float32)\n",
      "  Parameter dense1_bias (shape=(10,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net.collect_params)\n",
    "print(net.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "Note that while the Parameter objects exist,\n",
    "the input dimension to each layer is listed as `-1`.\n",
    "MXNet uses the special value `-1` to indicate\n",
    "that the parameters dimension remains unknown.\n",
    "At this point, attempts to access `net[0].weight.data()`\n",
    "would trigger a runtime error stating that the network\n",
    "must be initialized before the parameters can be accessed.\n",
    "Now let us see what happens when we attempt to initialize\n",
    "parameters via the `initialize` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential0_ (\n",
       "  Parameter dense0_weight (shape=(256, -1), dtype=float32)\n",
       "  Parameter dense0_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense1_weight (shape=(10, -1), dtype=float32)\n",
       "  Parameter dense1_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "As we can see, nothing has changed.\n",
    "When input dimensions are unknown,\n",
    "calls to initialize do not truly initialize the parameters.\n",
    "Instead, this call registers to MXNet that we wish\n",
    "(and optionally, according to which distribution)\n",
    "to initialize the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Net let us pass data through the network\n",
    "to make the framework finally initialize parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential0_ (\n",
       "  Parameter dense0_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense0_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense1_weight (shape=(10, 256), dtype=float32)\n",
       "  Parameter dense1_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.uniform(size=(2, 20))\n",
    "net(x)\n",
    "\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "As soon as we know the input dimensionality,\n",
    "$\\mathbf{x} \\in \\mathbb{R}^{20}$,\n",
    "the framework can identify the shape of the first layer's weight matrix,\n",
    "i.e., $\\mathbf{W}_1 \\in \\mathbb{R}^{256 \\times 20}$.\n",
    "Having recognized the first layer shape, the framework proceeds\n",
    "to the second layer, whose dimensionality is $10 \\times 256$\n",
    "and so on through the computational graph\n",
    "until all shapes are known.\n",
    "Note that in this case,\n",
    "only the first layer requires deferred initialization,\n",
    "but the framework initializes sequentially.\n",
    "Once all parameter shapes are known,\n",
    "the framework can finally initialize the parameters.\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Deferred initialization can be convenient, allowing the framework to infer parameter shapes automatically, making it easy to modify architectures and eliminating one common source of errors.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. What happens if you specify the input dimensions to the first layer but not to subsequent layers? Do you get immediate initialization?\n",
    "1. What happens if you specify mismatching dimensions?\n",
    "1. What would you need to do if you have input of varying dimensionality? Hint: look at parameter tying.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/280)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}