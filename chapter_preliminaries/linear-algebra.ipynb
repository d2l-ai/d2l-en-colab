{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U mxnet-cu101mkl==1.6.0  # updating mxnet to at least v1.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形代数\n",
    "\n",
    "データを格納・操作できるようになったので、ほとんどのモデルを理解するために必要となる、基本的な線形代数の一部を簡単に見てみましょう。基本的な数学的対象、算術、線形代数の演算を紹介し、それらの数学的な表記とコードで実装する方法を示します。\n",
    "\n",
    "\n",
    "## スカラー\n",
    "\n",
    "これまで線形代数や機械学習を勉強したことがなかったとしたら、おそらく一度に1つの数だけを扱ってきたかもしれません。そして、小切手帳の帳尻を合わしたり、レストランでの支払いをしたことがあれば、数のペアを足したり、掛けたりするといった基本的な方法はご存知でしょう。パロアルトの気温が華氏52度というのを例にあげましょう。正式には、これらの値を*スカラー*と呼びます。この値を摂氏に変換したい場合 (温度測定する単位としてメートル法が採用するより賢明な方法)、$f$を$52$として、式$c=(f-32)*5/9$を評価します。この式において、$32$、$5$、および$9$の各項はスカラー値です。何らかの値を代入するためのプレースホルダー$c$と$f$は変数と呼ばれ、それらは未知のスカラー値を表します。\n",
    "\n",
    "この書籍では、スカラーを通常の小文字 ($x$、$y$、$z$) とする数学的表記を利用します。また、すべての (連続の) 実数値スカラーがとりうる空間を$\\mathcal{R}$と表します。便宜上、*空間*の厳密な説明は後で行いますが、今のところ、$x \\in \\mathcal{R}$ という表現は $x$ が実数値スカラーであることを示す公式な方法であることを覚えておいてください。同様に、$x, y \\in {0, 1}$ は $x$ と $y$ が $0$ または $1$ をとることを表しています。\n",
    "\n",
    "\n",
    "MXNetでは、1つの要素だけをもつ `ndarray` を作成することでスカラーを表します。以下のスニペットでは、2つのスカラーをインスタンス化し、加算、乗算、除算、べき乗など、見慣れた算術演算を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(5.), array(6.), array(1.5), array(9.))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import np, npx\n",
    "npx.set_np()\n",
    "\n",
    "x = np.array(3.0)\n",
    "y = np.array(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベクトル\n",
    "\n",
    "ベクトルは単にスカラー値のリストとして考えることができます。ベクトル内の各数値は、単一のスカラー値で構成されています。これらの値をベクトルの*要素*や*成分* (英語では *entries* や *components*) と呼びます。ベクトルがデータセットに含まれるデータ例を表す場合、ベクトルの値は実世界の意味をもっているといえます。たとえば、ローンの債務不履行のリスクを調査している場合、収入、雇用期間、過去の債務不履行の数などに対応する要素を持つベクトルに、各申請者を関連付けることができるでしょう。もし、病院の患者の心臓発作のリスクを調べる場合は、最新のバイタルサイン、コレステロール値、1日当たりの運動時間などからなるベクトルで、患者の状態を表すかもしれません。数学表記では、通常、太字の小文字でベクトル (例えば、$\\mathbf{x}$、$\\mathbf{y}$、$\\mathbf{z}$)を表します。\n",
    "\n",
    "\n",
    "In MXNet, we work with vectors via $1$-dimensional `ndarray`s. In general `ndarray`s can have arbitrary lengths, subject to the memory limits of your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to any element of a vector by using a subscript. For example, we can refer to the $i^\\mathrm{th}$ element of $\\mathbf{x}$ by $x_i$. Note that the element $x_i$ is a scalar, so we do not bold-face the font when referring to it. Extensive literature considers column vectors to be the default orientation of vectors, so does this book. In math, a vector $\\mathbf{x}$ can be written as\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
    "\n",
    ":eqlabel:`eq_vec_def`\n",
    "\n",
    "\n",
    "ここで $x_1, \\ldots, x_n$ はベクトルの要素です。\n",
    "コードでは、`ndarray` にインデックスを付けることで任意の要素$i$にアクセスします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 長さ、次元、shape\n",
    "\n",
    ":numref:`sec_ndarray` からいくつかの概念を見直してみましょう。ベクトルは単に数値の配列です。そして、すべての配列が長さをもつのと同じように、すべてのベクトルも長さをもっています。ベクトル$\\mathbf{x}$が$n$個の実数値スカラーで構成されているとき、数学的な表記を用いて、これを$\\mathbf{x} \\in \\mathcal{R}^n$のように表現することができます。ベクトルの長さは通常、*次元*と呼ばれます。\n",
    "\n",
    "通常のPython配列と同様に、Pythonの組み込み関数``len()``を呼び出すことで `ndarray` の長さにアクセスできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ndarray` が (1軸で構成される) ベクトルを表すとき、\n",
    "`.shape`属性を利用することで、ベクトルの長さにアクセスすることもできます。shapeは、`ndarray` の各軸に沿った長さ (次元) をリスト形式で表現するタプルです。1つの軸だけをもつ `ndarray` において、shapeはたった1つの要素をもちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英語では次元をdimensionといいますが、これが様々な意味をもつがゆえに、人々を混乱させる傾向にあります。そこで、*dimensionality*という単語を使って、ベクトルまたは軸の*dimensionality*で長さ(つまりは要素数)を指すことがあります。しかし、`ndarray の`*dimensionality*は、`ndarray`がもつ軸の数を指すこともあります。この意味においては、`ndarray` の軸の *dimensionality* が軸の長さに相当するでしょう。\n",
    "\n",
    "\n",
    "## 行列\n",
    "\n",
    "ベクトルがスカラーを0次から1次に一般化したもののように、行列はベクトルを$1$次元から$2$次元に一般化したものになります。通常、大文字 の太字 (例えば、$X$、$Y$、$Z$) で表す行列は、コードのなかでは2つの軸をもつ`ndarray`として表されます。\n",
    "\n",
    "\n",
    "In math notation, we use $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "to express that the matrix $\\mathbf{A}$ consists of $m$ rows and $n$ columns of real-valued scalars.\n",
    "Visually, we can illustrate any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ as a table,\n",
    "where each element $a_{ij}$ belongs to the $i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column:\n",
    "\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    ":eqlabel:`eq_matrix_def`\n",
    "\n",
    "\n",
    "\n",
    "For any $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the shape of $\\mathbf{A}$\n",
    "is ($m$, $n$) or $m \\times n$.\n",
    "Specifically, when a matrix has the same number of rows and columns,\n",
    "its shape becomes a square; thus, it is called a *square matrix*.\n",
    "\n",
    "We can create an $m \\times n$ matrix in MXNet\n",
    "by specifying a shape with two components $m$ and $n$\n",
    "when calling any of our favorite functions for instantiating an `ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.],\n",
       "       [12., 13., 14., 15.],\n",
       "       [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.arange(20).reshape(5,4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行 ($i$) と列 ($j$) のインデックスを指定することで、行列$A$のスカラー要素$a_{ij}$にアクセスすることができます。 `:`を利用してインデックスを指定しなければ、それぞれの次元に沿ってすべての要素をとることができます (前の節で説明しました)。\n",
    "\n",
    "When the scalar elements of a matrix $\\mathbf{A}$, such as in :eqref:`eq_matrix_def`, are not given,\n",
    "we may simply use the lower-case letter of the matrix $\\mathbf{A}$ with the index subscript, $a_{ij}$,\n",
    "to refer to $[\\mathbf{A}]_{ij}$.\n",
    "To keep notation simple, commas are inserted to separate indices only when necessary,\n",
    "such as $a_{2, 3j}$ and $[\\mathbf{A}]_{2i-1, 3}$.\n",
    "\n",
    "\n",
    "Sometimes, we want to flip the axes.\n",
    "When we exchange a matrix's rows and columns,\n",
    "the result is called the *transpose* of the matrix.\n",
    "Formally, we signify a matrix $\\mathbf{A}$'s transpose by $\\mathbf{A}^\\top$\n",
    "and if $\\mathbf{B} = \\mathbf{A}^\\top$, then $b_{ij} = a_{ji}$ for any $i$ and $j$.\n",
    "Thus, the transpose of $\\mathbf{A}$ in :eqref:`eq_matrix_def` is\n",
    "a $n \\times m$ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^\\top =\n",
    "\\begin{bmatrix}\n",
    "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
    "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In code, we access a matrix's transpose via the `T` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  4.,  8., 12., 16.],\n",
       "       [ 1.,  5.,  9., 13., 17.],\n",
       "       [ 2.,  6., 10., 14., 18.],\n",
       "       [ 3.,  7., 11., 15., 19.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a special type of the square matrix,\n",
    "a *symmetric matrix* $\\mathbf{A}$ is equal to its transpose:\n",
    "$\\mathbf{A} = \\mathbf{A}^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [2., 0., 4.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列は便利なデータ構造です。行列を使用することで、異なる様々なデータを1つのデータとして構成することができます。たとえば、行列の行が各異なる家 (データ点) に対応し、列が異なる属性に対応します。This should sound familiar if you have ever used spreadsheet software or\n",
    "have read :numref:`sec_pandas`.\n",
    "Thus, although the default orientation of a single vector is a column vector,\n",
    "in a matrix that represents a tabular dataset,\n",
    "it is more conventional to treat each data point as a row vector in the matrix.\n",
    "And, as we will see in later chapters,\n",
    "this convention will enable common deep learning practices.\n",
    "For example, along the outermost axis of an `ndarray`,\n",
    "we can access or enumerate minibatches of data points,\n",
    "or just data points if no minibatch exists.\n",
    "\n",
    "## テンソル\n",
    "\n",
    "ベクトルがスカラーの一般化であるように、また、行列がベクトルの一般化であるように、さらに多くの軸をもつデータ構造を作成することができます。テンソルは、任意の数の軸を持つ`ndarray`を記述する汎用的な方法を提供しています。たとえば、ベクトルは1次テンソル、行列は2次テンソルです。Tensors are denoted with capital letters of a special font face\n",
    "(e.g., $\\mathsf{X}$, $\\mathsf{Y}$, and $\\mathsf{Z}$)\n",
    "and their indexing mechanism (e.g., $x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$) is similar to that of matrices.\n",
    "\n",
    "画像を扱い始める際には、テンソルはより重要なものとなります。なぜなら画像は、高さ、幅、カラーチャンネル (RGB) の3軸をもつ `ndarray`だからです。しかしこの章では、さらに高次のテンソルについてはスキップして、基本的な事項に注目します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (2, 3, 4)\n",
      "X = [[[ 0.  1.  2.  3.]\n",
      "  [ 4.  5.  6.  7.]\n",
      "  [ 8.  9. 10. 11.]]\n",
      "\n",
      " [[12. 13. 14. 15.]\n",
      "  [16. 17. 18. 19.]\n",
      "  [20. 21. 22. 23.]]]\n"
     ]
    }
   ],
   "source": [
    "X = np.arange(24).reshape((2, 3, 4))\n",
    "print('X.shape =', X.shape)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テンソル計算の基本的性質\n",
    "\n",
    "スカラー、ベクトル、行列、そして任意の次数のテンソルは、頼りになる良い性質をもっています。たとえば、elementwiseな演算の定義で気付いた方もいるかもしれませんが、同じshapeの計算対象が与えられた場合、elementwiseな演算の結果は同じshapeのテンソルになります。\n",
    "Similarly, given any two tensors with the same shape,\n",
    "the result of any binary elementwise operation\n",
    "will be a tensor of that same shape.\n",
    "For example, adding two matrices of the same shape\n",
    "performs elementwise addition over these two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]]),\n",
       " array([[ 0.,  2.,  4.,  6.],\n",
       "        [ 8., 10., 12., 14.],\n",
       "        [16., 18., 20., 22.],\n",
       "        [24., 26., 28., 30.],\n",
       "        [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.arange(20).reshape(5, 4)\n",
    "B = A.copy()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, elementwise multiplication of two matrices is called their *Hadamard product* (math notation $\\odot$).\n",
    "Consider matrix $\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ whose element of row $i$ and column $j$ is $b_{ij}$. The Hadamard product of matrices $\\mathbf{A}$ (defined in :eqref:`eq_matrix_def`) and $\\mathbf{B}$\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   1.,   4.,   9.],\n",
       "       [ 16.,  25.,  36.,  49.],\n",
       "       [ 64.,  81., 100., 121.],\n",
       "       [144., 169., 196., 225.],\n",
       "       [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying or adding a tensor by a scalar also does not change the shape of the tensor,\n",
    "where each element of the operand tensor will be added or multiplied by the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13.]],\n",
       " \n",
       "        [[14., 15., 16., 17.],\n",
       "         [18., 19., 20., 21.],\n",
       "         [22., 23., 24., 25.]]]),\n",
       " (2, 3, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = np.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 縮約\n",
    "\n",
    "任意のテンソルに対する演算で有用なものといえば、要素の合計を計算することでしょう。数学的表記では、合計を$\\sum$記号を使って表現します。長さ$d$のベクトル$\\mathbf{x}$の要素の合計を表すために$\\sum_{i=1}^d x_i$と書くことができます。コード上は、`sum`の関数を呼び出すだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3.]), array(6.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意のshapeをもつテンソルの要素についても総和を計算することができます。たとえば、$m \\times n$の行列　$\\mathbf{A}$　の要素の合計は、$\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$と書くことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 4), array(190.))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, invoking the `sum` function *reduces* a tensor along all its axes to a scalar.\n",
    "We can also specify the axes along which the tensor is reduced via summation.\n",
    "Take matrices as an example.\n",
    "To reduce the row dimension (axis $0$) by summing up elements of all the rows,\n",
    "we specify `axis=0` when invoking `sum`.\n",
    "Since the input matrix reduces along axis $0$ to generate the output vector,\n",
    "the dimension of axis $0$ of the input is lost in the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([40., 45., 50., 55.]), (4,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying `axis=1` will reduce the column dimension (axis $1$) by summing up elements of all the columns.\n",
    "Thus, the dimension of axis $1$ of the input is lost in the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6., 22., 38., 54., 70.]), (5,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing a matrix along both rows and columns via summation\n",
    "is equivalent to summing up all the elements of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(190.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1])  # Same as A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関連する計算として*平均(mean)*があります。英語では*mean*以外に*average*とも呼ばれます。合計を要素の数で割ることで平均を計算します。コードでは、任意の形のテンソルに `mean` を呼び出すだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(9.5), array(9.5))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `sum`, `mean` can also reduce a tensor along the specified axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8.,  9., 10., 11.]), array([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Reduction Sum\n",
    "\n",
    "However, sometimes it can be useful to keep the number of axes unchanged when invoking `sum` or `mean` by setting `keepdims=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.],\n",
       "       [22.],\n",
       "       [38.],\n",
       "       [54.],\n",
       "       [70.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, since `sum_A` still keeps its $2$ axes after summing each row, we can divide `A` by `sum_A` with broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.16666667, 0.33333334, 0.5       ],\n",
       "       [0.18181819, 0.22727273, 0.27272728, 0.3181818 ],\n",
       "       [0.21052632, 0.23684211, 0.2631579 , 0.28947368],\n",
       "       [0.22222222, 0.24074075, 0.25925925, 0.2777778 ],\n",
       "       [0.22857143, 0.24285714, 0.25714287, 0.27142859]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to calculate the cumulative sum of elements of `A` along some axis, say `axis=0` (row by row),\n",
    "we can call the `cumsum` function. This function will not reduce the input tensor along any axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  6.,  8., 10.],\n",
       "       [12., 15., 18., 21.],\n",
       "       [24., 28., 32., 36.],\n",
       "       [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ドット積\n",
    "\n",
    "これまでのところ、elementwiseな演算、合計、平均のみを扱ってきました。もし、これだけしかできないのであれば、線形代数として1つの章を設けるほどではないでしょう。ここで紹介したいのが、最も基本的な演算の1つであるドット積です。 2つのベクトル$\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$ が与えられたとき、それらの*ドット積* $\\mathbf{x}^T \\mathbf{y}$ (または $\\langle \\mathbf{x}, \\mathbf{y}  \\rangle$) は、同じ位置の要素の積の和となります。すなわち、$\\mathbf{x}^T \\mathbf{y} = \\sum_{i=1}^{d} x_i \\cdot y_i$ となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3.]), array([1., 1., 1., 1.]), array(6.))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.ones(4)\n",
    "x, y, np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つのベクトルのドット積は、要素ごとにelementwiseな乗算を実行して、その総和をとることと等価です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(6.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ドット積の有用性は幅広いです。たとえば、ベクトル $\\mathbf{x}  \\in \\mathbb{R}^d$ で表現される値の集合と、重みの集合 $\\mathbf{w} \\in \\mathbb{R}^d$ が与えられたとき、重み $\\mathbf{w}$ による $\\mathbf{x}$の値の重み付け和は、ドット積$\\mathbf{u}^T \\mathbf{w}$として表すことができます。重みが負ではなく、その総和が1になる場合$\\left(\\sum_{i=1}^{d} {w_i}=1 \\right)$、ドット積は*加重平均*を表します。2つのベクトルがそれぞれ長さ1になるように正規化されているとき、ドット積はそれらの間のコサイン角度を表します。この節では後ほど、*長さ*が何を意味するのかを説明します。\n",
    "\n",
    "## 行列ベクトル積\n",
    "\n",
    "ドット積の計算方法がわかったところで、*行列ベクトル積*についても理解しましょう。行列$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$とベクトル$\\mathbf{x} \\in \\mathbb{R}^n$ を定義して、eqref:`eq_matrix_def` と :eqref:`eq_vec_def` で見てみましょう。まず、行ベクトルの視点から行列 $\\mathbf{A}$ を見てみましょう。\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "それぞれ、$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^{n}$は、行列$A$の$i$番目の行を表す行ベクトルです。\n",
    "行列ベクトル積$\\mathbf{A}\\mathbf{x}$は、$i^\\mathrm{th}$番目の要素がドット積 $\\mathbf{a}^\\top_i \\mathbf{x}$となる長さ $m$ の列ベクトルとなります。\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "行列$\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$による乗算は、ベクトルを$\\mathbb{R}^{n}$から$\\mathbb{R}^{m}$への変換として考えることができます。\n",
    "\n",
    "これらの変換は非常に便利ですあることがわかります。たとえば回転という変換は、ある正方行列による乗算として表すことができます。以降の章で見られるように、前の層の値からニューラルネットワークの各層を計算する際、必要となるそれらの大量の計算を記述する際に、行列ベクトル積を使います。\n",
    "\n",
    "行列ベクトル積を`ndarray`を利用してコード内で表現するには、ドット積と同じ`dot`関数を使います。行列`Aとベクトル`xを指定して `np.dot(A, x)` を呼び出すと、行列ベクトル積が実行されます。 A`の列次元 (その長さは軸$1$方向のもの) は `x`の次元 (長さ) と同じでなければならないことに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 4), (4,), array([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, np.dot(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行列同士の積\n",
    "\n",
    "もし、ドット積と行列ベクトル積を理解できたとしたら、行列同士の積も同様に理解できるでしょう。\n",
    "\n",
    "2つの行列$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ と $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$を考えます。\n",
    "\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$ で、行列 $\\mathbf{A}$ の $i$ 番目の行の行ベクトルを表し、 $\\mathbf{b}_{j} \\in \\mathbb{R}^k$ で、行列 $\\mathbf{B}$ の $j$ 番目の列の列ベクトルを表します。\n",
    "行列積 $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$ を計算するには、行ベクトルに関して$\\mathbf{A}$を考えて、列ベクトルに関して$\\mathbf{B}$ を考えるのが最も簡単でしょう。\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix},\n",
    "\\quad B=\\begin{pmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "ここで行列積  $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$ は、各要素$\\mathbf{C}_{ij}$ はドット積 $\\mathbf{a}^\\top_i \\mathbf{b}_j$として計算されます。\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "$\\mathbf{AB}$の行列積については、単純に$m$個の行列ベクトル積を実行し、$n \\times m$ の行列になるように、その結果をつなげていく操作とみなすことができます。ドット積や行列ベクトル積と同様に、`dot`関数を利用して行列積を計算することができます。\n",
    "In the following snippet, we perform matrix multiplication on `A` and `B`.\n",
    "Here, `A` is a matrix with $5$ rows and $4$ columns,\n",
    "and `B` is a matrix with $4$ rows and $3$ columns.\n",
    "After multiplication, we obtain a matrix with $5$ rows and $3$ columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  6.,  6.],\n",
       "       [22., 22., 22.],\n",
       "       [38., 38., 38.],\n",
       "       [54., 54., 54.],\n",
       "       [70., 70., 70.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.ones(shape=(4, 3))\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-matrix multiplication can be simply called *matrix multiplication*, and should not be confused with the Hadamard product.\n",
    "\n",
    "\n",
    "## ノルム\n",
    "\n",
    "線形代数で最も有用な演算子であるノルムです。簡単に言えば、ベクトルのノルムは、ベクトルがどれくらい*大きい*かを示します。ここで考慮される*サイズ*は dimensionality ではなく、むしろ要素の大きさを表します。\n",
    "\n",
    "線形代数では、ベクトルノルムはベクトルをスカラーにマップする関数 $f$ であり、扱いやすい性質を持っています。あらゆるベクトル $\\mathbf{x}$ に対して成り立つ最初の性質は、ベクトルの要素をすべて定数 $\\alpha$ 倍したとき、そのノルムも同じ定数の*絶対値*で倍にしたものになります。\n",
    "\n",
    "\n",
    "$$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$$\n",
    "\n",
    "次の性質は三角不等式です。\n",
    "\n",
    "\n",
    "$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$$\n",
    "\n",
    "3つ目の性質は、単にノルムは非負でなければならないというものです。\n",
    "\n",
    "$$f(\\mathbf{x}) \\geq 0.$$\n",
    "\n",
    "これは、最小の*サイズ*が0であるという意味において納得できる性質です。最後の性質は、最小のノルムは全てのベクトルの要素がゼロであることと必要十分であることです。\n",
    "\n",
    "$$\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$$\n",
    "\n",
    "\n",
    "ひょっとするとノルムは距離の尺度の一種のように思えるかもしれません。\n",
    "あなたが小学校で学んだユークリッド距離 (ピタゴラスの定理を思い浮かべてください) を覚えていれば、そこから非負性と三角不等式について気づくかもしれません。\n",
    "実際のところ、ユークリッド距離はノルムで、具体的には$\\ell_2$ノルムです。\n",
    "Suppose that the elements in the $n$-dimensional vector\n",
    "$\\mathbf{x}$ are $x_1, \\ldots, x_n$.\n",
    "The $\\ell_2$ *norm* of $\\mathbf{x}$ is the square root of the sum of the squares of the vector elements:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$\n",
    "\n",
    "where the subscript $2$ is often omitted in $\\ell_2$ norms, i.e., $\\|\\mathbf{x}\\|$ is equivalent to $\\|\\mathbf{x}\\|_2$. コードでは、$\\ell_2$ノルムを計算するためには`linalg.norm`を呼ぶだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.array([3, -4])\n",
    "np.linalg.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, we work more often\n",
    "with the squared $\\ell_2$ norm.\n",
    "You will also frequently encounter the $\\ell_1$ *norm*,\n",
    "which is expressed as the sum of the absolute values of the vector elements:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$\n",
    "\n",
    "As compared with the $\\ell_2$ norm,\n",
    "it is less influenced by outliers.\n",
    "To calculate the $\\ell_1$ norm, we compose\n",
    "the absolute value function with a sum over the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(7.)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the $\\ell_2$ norm and the $\\ell_1$ norm\n",
    "are special cases of the more general $\\ell_p$ *norm*:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n",
    "\n",
    "Analogous to $\\ell_2$ norms of vectors,\n",
    "the *Frobenius norm* of a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$\n",
    "is the square root of the sum of the squares of the matrix elements:\n",
    "\n",
    "$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$\n",
    "\n",
    "The Frobenius norm satisfies all the properties of vector norms.\n",
    "It behaves as if it were an $\\ell_2$ norm of a matrix-shaped vector. Invoking `linalg.norm` will calculate the Frobenius norm of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(6.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ノルムと目的関数\n",
    "\n",
    ":label:`subsec_norms_and_objectives`\n",
    "\n",
    "\n",
    "先に進みすぎることは本意ではないのですが、なぜこれらの概念が有用なのか知ってほしいと思います。深層学習では、最適化問題を解くことがよくあります。最適化問題においては、観測されたデータに割り当てられる確率を最大化したり、予測と真実の観測との間の距離を*最小化*します。つまり、類似アイテム (単語、商品、ニュース記事など) 間の距離を最小化したり、非類似アイテム間の距離を最大化するように、アイテムにベクトル表現を割り当てます。この目的関数は、おそらく深層学習のアルゴリズムの構成要素で (データと並んで) 最も重要で、多くの場合、ノルムで表されます。\n",
    "\n",
    "## 線形代数のさらに先\n",
    "\n",
    "In just this section,\n",
    "we have taught you all the linear algebra\n",
    "that you will need to understand\n",
    "a remarkable chunk of modern deep learning.\n",
    "There is a lot more to linear algebra\n",
    "and a lot of that mathematics is useful for machine learning.\n",
    "For example, matrices can be decomposed into factors,\n",
    "and these decompositions can reveal\n",
    "low-dimensional structure in real-world datasets.\n",
    "There are entire subfields of machine learning\n",
    "that focus on using matrix decompositions\n",
    "and their generalizations to high-order tensors\n",
    "to discover structure in datasets and solve prediction problems.\n",
    "But this book focuses on deep learning.\n",
    "And we believe you will be much more inclined to learn more mathematics\n",
    "once you have gotten your hands dirty\n",
    "deploying useful machine learning models on real datasets.\n",
    "So while we reserve the right to introduce more mathematics much later on,\n",
    "we will wrap up this section here.\n",
    "\n",
    "If you are eager to learn more about linear algebra,\n",
    "you may refer to either :numref:`sec_geometry-linear-algebric-ops`\n",
    "or other excellent resources :cite:`Strang.1993,Kolter.2008,Petersen.Pedersen.ea.2008`.\n",
    "\n",
    "\n",
    "## まとめ\n",
    "\n",
    "* Scalars, vectors, matrices, and tensors are basic mathematical objects in linear algebra.\n",
    "* Vectors generalize scalars, and matrices generalize vectors.\n",
    "* In the `ndarray` representation, scalars, vectors, matrices, and tensors have 0, 1, 2, and an arbitrary number of axes, respectively.\n",
    "* A tensor can be reduced along the specified axes by `sum` and `mean`.\n",
    "* Elementwise multiplication of two matrices is called their Hadamard product. It is different from matrix multiplication.\n",
    "* In deep learning, we often work with norms such as the $\\ell_1$ norm, the $\\ell_2$ norm, and the Frobenius norm.\n",
    "* We can perform a variety of operations over scalars, vectors, matrices, and tensors with `ndarray` functions.\n",
    "\n",
    "\n",
    "## 練習\n",
    "\n",
    "1. Prove that the transpose of a matrix $\\mathbf{A}$'s transpose is $\\mathbf{A}$: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$.\n",
    "1. Given two matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that the sum of transposes is equal to the transpose of a sum: $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$.\n",
    "1. Given any square matrix $\\mathbf{A}$, is $\\mathbf{A} + \\mathbf{A}^\\top$ always symmetric? Why?\n",
    "1. We defined the tensor `X` of shape ($2$, $3$, $4$) in this section. What is the output of `len(X)`?\n",
    "1. For a tensor `X` of arbitrary shape, does `len(X)` always correspond to the length of a certain axis of `X`? What is that axis?\n",
    "1. Run `A / A.sum(axis=1)` and see what happens. Can you analyze the reason?\n",
    "1. When traveling between two points in Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?\n",
    "1. Consider a tensor with shape ($2$, $3$, $4$). What are the shapes of the summation outputs along axis $0$, $1$, and $2$?\n",
    "1. Feed a tensor with 3 or more axes to the `linalg.norm` function and observe its output. What does this function compute for `ndarray`s of arbitrary shape?\n",
    "\n",
    "## [議論のための](https://discuss.mxnet.io/t/2317)QRコード\n",
    "\n",
    "![](../img/qr_linear-algebra.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}